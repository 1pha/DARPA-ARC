{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DARPA-ARC Notebook 4: fMRI Second Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precompute Permutations\n",
    "Based on intial calculations, we assume one full loop of WLS + TFCE will take ~17s. We will submit jobs of 100 iterations (approx. 30 minutes time on cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(47404)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "n_subj = 28\n",
    "n_permutations = 5000\n",
    "inc = 100\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Generate permutations.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "permutations = []\n",
    "while True:\n",
    "    arr = np.random.choice([1,-1],n_subj,replace=True)\n",
    "    if not np.any(np.apply_along_axis(np.array_equal, 0, permutations, arr)): \n",
    "        permutations.append(arr)\n",
    "    if len(permutations) >= n_permutations: \n",
    "        break \n",
    "        \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Save.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "permutations = np.array(permutations)\n",
    "index = np.arange(0,n_permutations+1,inc)\n",
    "for n, ix in enumerate(index[1:]): \n",
    "    np.save(os.path.join('fmri_second_levels', 'permutations', 'sign_flips_%s' %(n+1)), permutations[ix-inc:ix])\n",
    "            \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Surface Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from mne import read_label, read_surface, spatial_tris_connectivity, set_log_level\n",
    "set_log_level(verbose=False) \n",
    "subj_dir = '/space/lilli/1/users/DARPA-Recons/fscopy'\n",
    "mri_dir = '/autofs/space/lilli_002/users/DARPA-ARC/NN_bayes_2016/FINAL6/'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "label_dir = os.path.join(subj_dir, 'label', 'dkt40')\n",
    "\n",
    "roi_list = ['caudalanteriorcingulate', 'rostralanteriorcingulate', 'posteriorcingulate',\n",
    "            'superiorfrontal', 'medialorbitofrontal', 'rostralmiddlefrontal', 'caudalmiddlefrontal',\n",
    "            'parsopercularis', 'parstriangularis', 'parsorbitalis', 'lateralorbitofrontal', 'insula']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Make labels.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "\n",
    "for hemi in ['lh', 'rh']:\n",
    "    \n",
    "    ## Assemble and merge labels.\n",
    "    label = []\n",
    "    for roi in roi_list: label.append(read_label(os.path.join(label_dir,'%s-%s.label' %(roi,hemi))))\n",
    "    label = np.sum(label)\n",
    "    \n",
    "    ## Save label.\n",
    "    label.name = 'arc-%s' %hemi\n",
    "    label.save('fmri_second_levels/arc-%s.label' %hemi)\n",
    "    \n",
    "    ## Load surface.\n",
    "    _, tris = read_surface(os.path.join(subj_dir, 'surf', '%s.white' %hemi))\n",
    "    mapping = np.in1d(np.unique(tris),label.vertices)\n",
    "    \n",
    "    ## Reduce triangles to those in label.\n",
    "    ix = np.all(np.apply_along_axis(np.in1d, 0, tris, label.vertices), axis=1)\n",
    "    tris = tris[ix]\n",
    "\n",
    "    ## Compute connectivity.\n",
    "    coo = spatial_tris_connectivity(tris, remap_vertices=True)\n",
    "    np.savez('fmri_second_levels/%s_connectivity' %hemi, data = coo.data, row = coo.row,\n",
    "             col = coo.col, shape = coo.shape, mapping=mapping, vertices=label.vertices)\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Volume Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from scipy.sparse import coo_matrix\n",
    "lut = '/usr/local/freesurfer/stable5_3_0/FreeSurferColorLUT.txt'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define subcortical structures of interest.\n",
    "roi_dict = {18:'Left-Amygdala', 11:'Left-Caudate', 17:'Left-Hippocampus', 12:'Left-Putamen', \n",
    "            54:'Right-Amygdala', 50:'Right-Caudate', 53:'Right-Hippocampus', 51:'Right-Putamen'}\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Create mask.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load aseg.\n",
    "aseg = '/space/lilli/1/users/DARPA-Recons/fsaverage/mri.2mm/aseg.mgz'\n",
    "aseg = nib.load(aseg).get_data()\n",
    "\n",
    "## Find all voxels in ROI list. Get corresponding labels.\n",
    "mapping = np.in1d(aseg, roi_dict.keys()).reshape(aseg.shape)\n",
    "voxels = np.where(mapping)\n",
    "names = np.array([roi_dict[i] for i in aseg[voxels]])\n",
    "voxels = np.vstack(voxels).T\n",
    "\n",
    "## Initialize connectivity matrix.\n",
    "n_voxels, _ = voxels.shape\n",
    "coo = np.zeros([n_voxels,n_voxels], dtype=int)\n",
    "\n",
    "## Iteratively test for adjacency.\n",
    "## Here we use 6-lattice connectivity (up,down,forward,backward,left,right).\n",
    "for n in range(n_voxels):\n",
    "    diff = np.linalg.norm(voxels - voxels[n], axis=1)\n",
    "    M, = np.where(diff==1.)\n",
    "    for m in M: coo[n,m] = 1 \n",
    "coo = coo_matrix(coo)\n",
    "    \n",
    "## Save.\n",
    "np.savez('fmri_second_levels/mni305_connectivity', data = coo.data, row = coo.row,\n",
    "         col = coo.col, shape = coo.shape, mapping=mapping, voxels=voxels, names=names)\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Mean Signal from ROIs\n",
    "Necessary for computing percent signal change down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pandas import read_csv\n",
    "mri_dir = 'fmri_first_levels/'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "\n",
    "n_acq = 977\n",
    "tr = 1.75\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Get subjects list.\n",
    "info = read_csv('demographics.csv')\n",
    "subjects = info.loc[~info.Exlude, 'Subject'].as_matrix()\n",
    "\n",
    "## Define TR onsets.\n",
    "tr_onsets = np.insert( np.cumsum( np.ones(n_acq - 1) * tr ), 0, 0 )\n",
    "\n",
    "mean_signal = dict()\n",
    "for space in ['lh','rh','mni305']:\n",
    "    \n",
    "    print space,\n",
    "    \n",
    "    ## Load masks.\n",
    "    npz = np.load('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "    include = npz['mapping']\n",
    "    \n",
    "    ## Preallocate space.\n",
    "    ms = np.zeros([len(subjects), include.sum()])\n",
    "    \n",
    "    ## Iterate over subjects.\n",
    "    for n, subject in enumerate(subjects):\n",
    "        \n",
    "        ## Load data.\n",
    "        subj_dir = os.path.join(mri_dir, subject, 'arc_001', '001')\n",
    "        if space == 'mni305': f = os.path.join(subj_dir,'fmcpr.sm%s.%s.2mm.b0dc.nii.gz' %(sm,space))\n",
    "        else: f = os.path.join(subj_dir,'fmcpr.sm%s.fsaverage.%s.b0dc.nii.gz' %(sm,space))\n",
    "        data = nib.load(f).get_data()\n",
    "        \n",
    "        ## Censor data. Average across acquisitions.\n",
    "        try: censor = np.loadtxt(os.path.join(subj_dir, 'FINAL.censor.%s.par' %fd))\n",
    "        except IOError: censor = []\n",
    "        censor = np.invert(np.in1d(tr_onsets, censor))\n",
    "        \n",
    "        data = data[include,...].squeeze()\n",
    "        data = data[...,censor].mean(axis=1)\n",
    "        \n",
    "        ## Append.\n",
    "        ms[n] = data\n",
    "    \n",
    "    ## Store in dictionary.\n",
    "    mean_signal[space] = ms\n",
    "    \n",
    "## Save.\n",
    "f = 'fmri_second_levels/mean_signal'\n",
    "np.savez_compressed(f, lh = mean_signal['lh'], rh = mean_signal['rh'], mni305 = mean_signal['mni305'])\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from mne import read_label\n",
    "from pandas import read_csv\n",
    "mri_dir = 'fmri_first_levels/concat-sess/FINAL2/'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "sm = 6\n",
    "spaces = ['lh','rh','mni305']\n",
    "thresholds = [0.9]\n",
    "contrasts = ['Delib', 'DelibMod', 'Antcp', 'AntcpMod', 'Shock']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for space in spaces:\n",
    "\n",
    "    ## Load masks.\n",
    "    npz = np.load('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "    include = npz['mapping']\n",
    "    \n",
    "    for fd in thresholds:\n",
    "                \n",
    "        results_dir = os.path.join(mri_dir, 'FINAL2.%s.%s.%s' %(sm,fd,space))\n",
    "            \n",
    "        for contrast in contrasts:\n",
    "        \n",
    "            ## Make save directory.\n",
    "            out_dir = os.path.join('fmri_second_levels', 'FINAL2.%s.%s.%s.%s' %(sm,fd,space,contrast))\n",
    "            if not os.path.isdir(out_dir): os.makedirs(out_dir)\n",
    "    \n",
    "            ## Load data.\n",
    "            ces = nib.load(os.path.join(results_dir, 'FINAL2.%s.par' %contrast, 'ces.nii.gz')).get_data()\n",
    "            cesvar = nib.load(os.path.join(results_dir, 'FINAL2.%s.par' %contrast, 'cesvar.nii.gz')).get_data()\n",
    "            affine = nib.load(os.path.join(results_dir, 'FINAL2.%s.par' %contrast, 'ces.nii.gz')).affine\n",
    "            \n",
    "            ## Masking.\n",
    "            ces = ces[include,...]\n",
    "            cesvar = cesvar[include,...]\n",
    "                    \n",
    "            ## Save.\n",
    "            np.savez_compressed(os.path.join(out_dir, 'first_levels'), ces=ces.squeeze(), cesvar=cesvar.squeeze())\n",
    "            np.save(os.path.join(out_dir, 'affine'), affine)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform WLS Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from scipy.sparse import coo_matrix\n",
    "from mne.stats.cluster_level import _find_clusters as find_clusters\n",
    "root_dir = '/space/sophia/2/users/DARPA-Behavior/notebooks/bayes/decision_making/NN_bayes_2016'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "space = 'mni305'\n",
    "contrast = 'Delib'\n",
    "\n",
    "## Permutation parameters.\n",
    "permutations = 0\n",
    "\n",
    "## TFCE parameters.\n",
    "threshold = dict(start=0.1, step=0.1, h_power=2, e_power=0.5)\n",
    "tail = 0\n",
    "max_step = 1\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "def load_sparse_coo(filename):\n",
    "    npz = np.load(filename)\n",
    "    M,N = npz['shape']\n",
    "    return coo_matrix( (npz['data'], (npz['row'],npz['col'])), (M,N) )\n",
    "\n",
    "out_dir = os.path.join(root_dir, 'fmri_second_levels', 'FINAL.%s.%s.%s.%s' %(sm,fd,space,contrast))\n",
    "\n",
    "## Load data.\n",
    "npz = np.load(os.path.join(out_dir, 'first_levels.npz'))\n",
    "ces = npz['ces']\n",
    "cesvar = np.abs( 1. / npz['cesvar'] )\n",
    "\n",
    "## Define indices.\n",
    "connectivity = load_sparse_coo(os.path.join(root_dir, 'fmri_second_levels', '%s_connectivity.npz' %space))\n",
    "index,  = np.where(~np.isinf(cesvar).sum(axis=1).astype(bool))\n",
    "include = ~np.isinf(cesvar).sum(axis=1).astype(bool)\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup for permutation testing.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load subject information.\n",
    "info = read_csv(os.path.join(root_dir, 'demographics.csv'))\n",
    "info = info[~info.Exlude].reset_index()\n",
    "n_subj, _ = info.shape\n",
    "\n",
    "## Build Design Matrix.\n",
    "X = np.zeros((n_subj,2))\n",
    "X[:,0] = 1                                        # Intercept\n",
    "X[:,1] = np.where(info.Scanner == 'Trio', 0, 1)   # Scanner\n",
    "n_subj, n_pred = X.shape\n",
    "\n",
    "## If specified, load precomputed sign flips.\n",
    "if permutations: sign_flips = np.load(os.path.join(root_dir, 'fmri_second_levels', 'permutations', 'sign_flips_%s.npy' %permutations))\n",
    "else: sign_flips = np.ones((1,n_subj))\n",
    "n_shuffles = sign_flips.shape[0]\n",
    "\n",
    "## Preallocate arrays for results.\n",
    "shape = [n_shuffles] + list(ces.shape[:-1])\n",
    "Bmap = np.zeros(shape)\n",
    "Fmap = np.zeros(shape)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "'''\n",
    "Following the instructions of Winkler et al. (2014), we use the Freedman and Lane (1983)\n",
    "permutation procedure. This allows us to precompute a number of values ahead of time.\n",
    "\n",
    "To understand the WLS computations, please see:\n",
    "https://github.com/statsmodels/statsmodels/blob/master/statsmodels/base/model.py\n",
    "https://github.com/statsmodels/statsmodels/blob/master/statsmodels/regression/linear_model.py\n",
    "'''\n",
    "\n",
    "def wls(X,Y,W):\n",
    "    B = np.linalg.inv(X.T.dot(W).dot(X)).dot(X.T).dot(W).dot(Y)\n",
    "    ssr = W.dot( np.power(Y - np.dot(X,B),2) ).sum()\n",
    "    scale = ssr / (n_subj - n_pred)\n",
    "    cov_p = np.linalg.inv(X.T.dot(W).dot(X)) * scale\n",
    "    F = np.power(B[0],2) * np.power(cov_p[0,0],-1)\n",
    "    return B[0], F\n",
    "\n",
    "## Loop it!\n",
    "for n, sf in enumerate(sign_flips):\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute statistics.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    for m in index:\n",
    "\n",
    "        ## Update variables.\n",
    "        W = np.diag(cesvar[m])\n",
    "        Y = ces[m]\n",
    "        \n",
    "        ## Permute values.\n",
    "        ## See Winkler et al. (2014), pg. 385\n",
    "        ## To compute Hat Matrix, see: https://en.wikipedia.org/wiki/Projection_matrix and \n",
    "        Z = X[:,1:]\n",
    "        ZZ = Z.dot( np.linalg.inv( Z.T.dot(W).dot(Z) ) ).dot(Z.T).dot(W)\n",
    "        Rz = np.identity(n_subj) - ZZ\n",
    "        Y = np.diag(sf).dot(Rz).dot(Y)\n",
    "        \n",
    "        ## Perform WLS.\n",
    "        Bmap[n,m], Fmap[n,m] = wls(X,Y,W) \n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Perform TFCE.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    _, Fmap[n] = find_clusters(Fmap[n], threshold, tail=tail, connectivity=connectivity, \n",
    "                               include=include, max_step=max_step, show_info=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Save results.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#  \n",
    "\n",
    "if not permutations: f = os.path.join(out_dir, '%s_%s_obs' %(space, contrast))\n",
    "else: f = os.path.join(out_dir, '%s_%s_perm-%s' %(space, contrast, permutations)) \n",
    "np.savez_compressed(f, Bmap=Bmap, Fmap=Fmap)\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform FWE Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "def prepare_image(arr, space):\n",
    "    npz = np.load('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "    image = np.zeros_like(npz['mapping'], dtype=float)\n",
    "    \n",
    "    if not space == 'mni305': \n",
    "        image[npz['vertices']] += arr\n",
    "    else:\n",
    "        x,y,z = npz['voxels'].T\n",
    "        image[x,y,z] += arr\n",
    "    \n",
    "    for _ in range(4 - len(image.shape)): image = np.expand_dims(image,-1)\n",
    "    return image\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "spaces = ['lh','rh','mni305']\n",
    "contrasts = ['Delib','DelibMod','Antcp','AntcpMod','Shock']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "permutations = np.arange(50) + 1\n",
    "\n",
    "for contrast in contrasts:\n",
    "\n",
    "    print contrast,\n",
    "    \n",
    "    for n, space in enumerate(spaces):\n",
    "    \n",
    "        out_dir = 'fmri_second_levels/FINAL2.%s.%s.%s.%s' %(sm, fd, space, contrast)\n",
    "        \n",
    "        ## Load true effects.\n",
    "        npz = np.load(os.path.join(out_dir, '%s_%s_obs.npz' %(space,contrast)))\n",
    "        Bmap = npz['Bmap'].squeeze()\n",
    "        Fmap = npz['Fmap'].squeeze()\n",
    "        \n",
    "        ## Load permutations.\n",
    "        Pmap = []\n",
    "        for p in permutations: \n",
    "            npz = np.load(os.path.join(out_dir, '%s_%s_perm-%s.npz' %(space,contrast,p)))\n",
    "            Pmap.append(npz['Fmap'])\n",
    "        Pmap = np.concatenate(Pmap, axis=0)\n",
    "        n_permutations, _ = Pmap.shape\n",
    "\n",
    "        ## Compute p-values via FWE.\n",
    "        p_values = np.ones_like(Fmap)\n",
    "        for mp in Pmap.max(axis=1): p_values += mp > Fmap\n",
    "        p_values /= n_permutations + 1.\n",
    "        p_values = -np.log10(p_values) * np.sign(Bmap)\n",
    "      \n",
    "        ## Save maps.\n",
    "        np.save(os.path.join(out_dir,'%s_%s_fwe' %(space,contrast)), p_values)\n",
    "        for arr, name in zip([Bmap,Fmap,p_values],['beta','F','fwe']):\n",
    "            image = prepare_image(arr, space)\n",
    "            image = nib.Nifti1Image(image, np.load(os.path.join(out_dir,'affine.npy')))\n",
    "            nib.save(image, os.path.join(out_dir, '%s.nii.gz' %name))\n",
    "            \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform FDR Corrections\n",
    "Not used, but programmed for example's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from mne.stats import fdr_correction\n",
    "\n",
    "def prepare_image(arr, space):\n",
    "    npz = np.load('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "    image = np.zeros_like(npz['mapping'], dtype=float)\n",
    "    \n",
    "    if not space == 'mni305': \n",
    "        image[npz['vertices']] += arr\n",
    "    else:\n",
    "        x,y,z = npz['voxels'].T\n",
    "        image[x,y,z] += arr\n",
    "    \n",
    "    for _ in range(4 - len(image.shape)): image = np.expand_dims(image,-1)\n",
    "    return image\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "spaces = ['lh','rh','mni305']\n",
    "contrasts = ['Delib','DelibMod','Antcp','AntcpMod','Shock']\n",
    "contrasts = ['DelibMod']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "permutations = np.arange(50) + 1\n",
    "\n",
    "for contrast in contrasts:\n",
    "\n",
    "    print contrast,\n",
    "    FDR, signs = [], []\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute p-values within spaces.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    for n, space in enumerate(spaces):\n",
    "    \n",
    "        out_dir = 'fmri_second_levels/FINAL.%s.%s.%s.%s' %(sm, fd, space, contrast)\n",
    "        \n",
    "        ## Load true effects.\n",
    "        npz = np.load(os.path.join(out_dir, '%s_%s_obs.npz' %(space,contrast)))\n",
    "        Bmap = npz['Bmap'].squeeze()\n",
    "        Fmap = npz['Fmap'].squeeze()\n",
    "        \n",
    "        ## Load permutations.\n",
    "        Pmap = []\n",
    "        for p in permutations: \n",
    "            npz = np.load(os.path.join(out_dir, '%s_%s_perm-%s.npz' %(space,contrast,p)))\n",
    "            Pmap.append(npz['Fmap'])\n",
    "        Pmap = np.concatenate(Pmap, axis=0)\n",
    "        n_permutations, _ = Pmap.shape\n",
    "\n",
    "        ## Compute p-values via FWE.\n",
    "        p_values = (Pmap >= Fmap).sum(axis=0) + 1.\n",
    "        p_values /= n_permutations + 1.\n",
    "        FDR.append(p_values)\n",
    "        signs.append(np.sign(Bmap))\n",
    "    \n",
    "        ## Save maps.\n",
    "        for arr, name in zip([Bmap,Fmap],['beta','F']):\n",
    "            image = prepare_image(arr, space)\n",
    "            image = nib.Nifti1Image(image, np.load(os.path.join(out_dir,'affine.npy')))\n",
    "            nib.save(image, os.path.join(out_dir, '%s.nii.gz' %name))        \n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Perform FDR corrections.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            \n",
    "    ## Assemble info.\n",
    "    indices = np.concatenate([np.ones_like(arr) * n for n, arr in enumerate(FDR)])\n",
    "    FDR = np.concatenate(FDR)\n",
    "    signs = np.concatenate(signs)\n",
    "\n",
    "    ## Perform FDR correction.\n",
    "    FDR[np.where(signs)] = fdr_correction(FDR[np.where(signs)])[-1]\n",
    "    FDR = -np.log10(FDR) * signs\n",
    "\n",
    "    ## Save maps.\n",
    "    for n, space in enumerate(spaces):\n",
    "        out_dir = 'fmri_second_levels/FINAL.%s.%s.%s.%s' %(sm, fd, space, contrast)\n",
    "        np.save(os.path.join(out_dir,'%s_%s_fdr' %(space,contrast)), FDR[indices==n])\n",
    "        image = prepare_image(FDR[indices==n], space)\n",
    "        image = nib.Nifti1Image(image, np.load(os.path.join(out_dir,'affine.npy')))\n",
    "        nib.save(image, os.path.join(out_dir, 'fdr.nii.gz'))\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Second-Level Maps\n",
    "Thresholding clusters such that:\n",
    "* p < 0.05 (FWE corrected, alpha = 0.05)\n",
    "* Surface: clusters > 100mm2\n",
    "* Volume: clusters > 20 contiguous voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pandas import DataFrame\n",
    "from scipy.sparse import coo_matrix\n",
    "from mne.stats.cluster_level import _find_clusters as find_clusters\n",
    "fs_dir = 'recons'\n",
    "\n",
    "def load_sparse_coo(filename):\n",
    "    npz = np.load(filename)\n",
    "    M,N = npz['shape']\n",
    "    return coo_matrix( (npz['data'], (npz['row'],npz['col'])), (M,N) )\n",
    "\n",
    "def prepare_image(arr, space):\n",
    "    npz = np.load('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "    image = np.zeros_like(npz['mapping'], dtype=float)\n",
    "    \n",
    "    if not space == 'mni305': \n",
    "        image[npz['vertices']] += arr\n",
    "    else:\n",
    "        x,y,z = npz['voxels'].T\n",
    "        image[x,y,z] += arr\n",
    "    \n",
    "    for _ in range(4 - len(image.shape)): image = np.expand_dims(image,-1)\n",
    "    return image\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "spaces = ['lh','rh','mni305']\n",
    "contrasts = ['Delib','DelibMod','Antcp','AntcpMod','Shock']\n",
    "\n",
    "## Thresholding parameters.\n",
    "threshold = -np.log10( 0.05 )\n",
    "min_cluster = dict(lh = 100, rh = 100, mni305 = 20)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for space in spaces:\n",
    "    \n",
    "    print space,\n",
    "    \n",
    "    ## Load connectivity information.\n",
    "    connectivity = load_sparse_coo('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "\n",
    "    ## Load mapping information.\n",
    "    npz = np.load('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "    \n",
    "    if not space == 'mni305':\n",
    "        vertices = npz['vertices']\n",
    "        average_area = nib.load(os.path.join(fs_dir, 'fsaverage', 'surf', '%s.white.avg.area.mgh' %space)).get_data()\n",
    "        average_area = average_area[vertices].squeeze()\n",
    "        \n",
    "    for contrast in contrasts:\n",
    "        \n",
    "        ## Load FWE-corrected p-values.\n",
    "        f = 'fmri_second_levels/FINAL2.%s.%s.%s.%s/%s_%s_fwe.npy' %(sm, fd, space, contrast, space, contrast)\n",
    "        fwe = np.load(f)\n",
    "        \n",
    "        ## Find clusters.\n",
    "        include = np.where(fwe,True,False)\n",
    "        clusters, sums = find_clusters(fwe, threshold, tail=0, connectivity=connectivity, include=include, t_power=0)\n",
    "        \n",
    "        ## Compute areas.\n",
    "        if not space == 'mni305': cluster_sums = np.array([average_area[c].sum() for c in clusters])\n",
    "        else: cluster_sums = sums\n",
    "            \n",
    "        ## Threshold.\n",
    "        try:\n",
    "            survival_ix = np.concatenate([c for c, s in zip(clusters,cluster_sums) if s > min_cluster[space]])\n",
    "            fwe[~np.in1d(np.arange(fwe.shape[0]), survival_ix)] = 0\n",
    "        except ValueError:\n",
    "            fwe = np.zeros_like(fwe)\n",
    "        \n",
    "        ## Save.\n",
    "        image = prepare_image(fwe, space)\n",
    "        image = nib.Nifti1Image(image, np.load(os.path.join(os.path.dirname(f),'affine.npy')))\n",
    "        nib.save(image, os.path.join(os.path.dirname(f), 'fwe_thresh_%0.3f.nii.gz' %threshold))\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Percent Signal Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pandas import read_csv\n",
    "mri_dir = 'fmri_first_levels'\n",
    "\n",
    "def prepare_image(arr, space):\n",
    "    npz = np.load('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "    image = np.zeros_like(npz['mapping'], dtype=float)\n",
    "    \n",
    "    if not space == 'mni305': \n",
    "        image[npz['vertices']] += arr\n",
    "    else:\n",
    "        x,y,z = npz['voxels'].T\n",
    "        image[x,y,z] += arr\n",
    "    \n",
    "    for _ in range(4 - len(image.shape)): image = np.expand_dims(image,-1)\n",
    "    return image\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "spaces = ['lh','rh','mni305']\n",
    "contrasts = ['Delib','DelibMod','Antcp','AntcpMod','Shock']\n",
    "threshold = 1.301\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Get subjects list.\n",
    "info = read_csv('demographics.csv')\n",
    "subjects = info.loc[~info.Exlude, 'Subject'].as_matrix()\n",
    "\n",
    "## Load average signal.\n",
    "mean_signal = np.load('fmri_second_levels/mean_signal.npz')\n",
    "\n",
    "for space in spaces:\n",
    "    \n",
    "    print space,\n",
    "    \n",
    "    ## Assemble design matrices.\n",
    "    subj_dir = os.path.join(mri_dir, '%s', 'arc_001', 'FINAL2.%s.%s.%s' %(sm, fd, space), 'X.dat')\n",
    "    scale_factors = np.array([np.loadtxt(subj_dir %subject).max(axis=0)[:len(contrasts)] \n",
    "                             for subject in subjects]).T\n",
    "\n",
    "    for n, contrast in enumerate(contrasts):\n",
    "        \n",
    "        ## Load first levels.\n",
    "        out_dir = 'fmri_second_levels/FINAL2.%s.%s.%s.%s' %(sm, fd, space, contrast)\n",
    "        ces = np.load(os.path.join(out_dir, 'first_levels.npz'))['ces']\n",
    "        \n",
    "        ## Compute PSC (Pernet 2014, Frontiers in Neuroscience).\n",
    "        ms = np.where(mean_signal[space], mean_signal[space], np.inf).T\n",
    "        psc = np.divide(ces * scale_factors[n] * 100., ms)\n",
    "        psc = prepare_image(psc.mean(axis=1), space)\n",
    "        \n",
    "        ## Mask image.\n",
    "        fwe = nib.load(os.path.join(out_dir, 'fwe_thresh_%s.nii.gz' %threshold)).get_data()\n",
    "        psc *= np.where(fwe,1,0)\n",
    "        \n",
    "        ## Save.\n",
    "        image = nib.Nifti1Image(psc, np.load(os.path.join(out_dir,'affine.npy')))\n",
    "        nib.save(image, os.path.join(out_dir, 'psc.nii.gz'))\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "from surfer import Brain\n",
    "img_dir = 'plots/FINAL2/second_levels'\n",
    "%matplotlib qt4\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "contrasts = ['Delib']\n",
    "overlay = 'psc'\n",
    "surface = 'inflated'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for hemi in ['lh','rh']: \n",
    "\n",
    "    for contrast in contrasts:\n",
    "\n",
    "        fn = os.path.join('fmri_second_levels', 'FINAL.%s.%s.%s.%s' %(sm, fd, hemi, contrast), '%s.nii.gz' %overlay)\n",
    "        \n",
    "        for view in ['lateral','medial']:\n",
    "            \n",
    "            brain = Brain(\"fsaverage\", hemi, surface)\n",
    "            try: brain.add_overlay(fn, min=0.04, max=2.5, sign=\"pos\")\n",
    "            except: continue\n",
    "            brain.show_view(view=view)\n",
    "            od = os.path.join(img_dir, overlay, surface)\n",
    "            if not os.path.isdir(od): os.makedirs(od)\n",
    "            of = os.path.join(od, '%s_%s_%s.png' %(hemi,contrast,view))\n",
    "            Brain.save_image(brain,of)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute surface summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from mne import Label, read_label, grow_labels, vertex_to_mni, set_log_level\n",
    "set_log_level(verbose=False)\n",
    "fs_dir = 'recons'\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "contrast = 'DelibMod'\n",
    "thresh = 1.301\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "\n",
    "## ROI parameters.\n",
    "extent = 10 #mm\n",
    "grow = False\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "label_dir = 'fmri_second_levels/labels/seeds_%s' %contrast\n",
    "labels = sorted([f for f in os.listdir(label_dir) if not f.startswith('fig') and f.endswith('label')])\n",
    "\n",
    "fmni = open('%s/%s_surface_mni2.csv' %(label_dir,contrast), 'w')\n",
    "fmni.write(','.join(['Label','V','X','Y','Z','PSC','F','p'])+'\\n')\n",
    "\n",
    "for f in labels:\n",
    "\n",
    "    ## Extract info. Read label.\n",
    "    roi, hemi = f.replace('.label', '').split('-')\n",
    "    label = read_label(os.path.join(label_dir, f))\n",
    "    \n",
    "    ## Load accompanying overlay.\n",
    "    f = 'fmri_second_levels/FINAL2.%s.%s.%s.%s/psc.nii.gz' %(sm,fd,hemi,contrast)\n",
    "    overlay = nib.load(f).get_data().squeeze()\n",
    "\n",
    "    ## Find maximum vertex.\n",
    "    ix = np.argmax(overlay[label.vertices])\n",
    "    v = label.vertices[ix]\n",
    "\n",
    "    ## Extract MNI coordinates.\n",
    "    x,y,z = vertex_to_mni(v, 0 if hemi=='lh' else 1, 'fsaverage', fs_dir)[0]\n",
    "    \n",
    "    ## Extract PSC, F-scores, p-values.\n",
    "    f = 'fmri_second_levels/FINAL2.%s.%s.%s.%s/psc.nii.gz' %(sm,fd,hemi,contrast)\n",
    "    psc = nib.load(f).get_data().squeeze()[v]\n",
    "\n",
    "    f = 'fmri_second_levels/FINAL2.%s.%s.%s.%s/F.nii.gz' %(sm,fd,hemi,contrast)\n",
    "    F = nib.load(f).get_data().squeeze()[v]\n",
    "    \n",
    "    f = 'fmri_second_levels/FINAL2.%s.%s.%s.%s/fwe_thresh_%s.nii.gz' %(sm,fd,hemi,contrast,thresh)\n",
    "    p = nib.load(f).get_data().squeeze()[v]\n",
    "    \n",
    "    ## Write information.\n",
    "    fmni.write('%s-%s,%s,%0.0f,%0.0f,%0.0f,%0.2f,%0.2f,%0.6f\\n' %(roi,hemi,v,x,y,z,psc,F,10.**-p))\n",
    "    \n",
    "    if grow:\n",
    "        \n",
    "        ## Grow label.\n",
    "        label = grow_labels('fsaverage', v, extent, 0 if hemi=='lh' else 1, subjects_dir=fs_dir,\n",
    "                            names='fig_%s-%s' %(roi,hemi), surface='pial')[0]\n",
    "        \n",
    "        ## Ensure label is within actiation. Save.\n",
    "        ix = np.in1d(label.vertices, np.where(overlay)[0])\n",
    "        label.pos = label.pos[ix]\n",
    "        label.values = label.values[ix]\n",
    "        label.vertices = label.vertices[ix]\n",
    "        label.save('%s/%s.label' %(label_dir, label.name))\n",
    "    \n",
    "fmni.close()\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute volume summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nibabel.affines import apply_affine\n",
    "fs_dir = '/space/lilli/1/users/DARPA-Recons'\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "contrast = 'Delib'\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "\n",
    "## ROI parameters.\n",
    "extent = 6 #mm\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "label_dir = 'fmri_second_levels/labels/seeds_%s' %contrast\n",
    "\n",
    "## Initialize statistics file.\n",
    "fmni = open('%s/%s_volume_mni2.csv' %(label_dir,contrast), 'w')\n",
    "fmni.write(','.join(['Label','cX','cY','cZ','X','Y','Z','PSC','F','p'])+'\\n')\n",
    "\n",
    "## Load data. \n",
    "npz = np.load('fmri_second_levels/mni305_connectivity.npz')\n",
    "affine = np.load('fmri_second_levels/FINAL2.%s.%s.mni305.%s/affine.npy' %(sm,fd,contrast))\n",
    "obj = nib.load('fmri_second_levels/FINAL2.%s.%s.mni305.%s/psc.nii.gz' %(sm,fd,contrast))\n",
    "\n",
    "overlay = obj.get_data().squeeze()\n",
    "Fval = nib.load('fmri_second_levels/FINAL2.%s.%s.mni305.%s/F.nii.gz' %(sm,fd,contrast)).get_data().squeeze()\n",
    "pval = nib.load('fmri_second_levels/FINAL2.%s.%s.mni305.%s/fwe_thresh_1.301.nii.gz' %(sm,fd,contrast)).get_data().squeeze()\n",
    "\n",
    "rois = ['Left-Caudate', 'Left-Putamen', 'Left-Hippocampus',\n",
    "        'Right-Caudate', 'Right-Putamen', 'Right-Hippocampus']\n",
    "for roi in rois:\n",
    "    \n",
    "    ## Extract activated voxels in ROI.\n",
    "    voxels = npz['voxels'][npz['names'] == roi]\n",
    "    voxels = voxels[np.where(overlay[[arr for arr in voxels.T]])]\n",
    "    \n",
    "    ## Find maximally activated voxel.\n",
    "    ix = np.argmax(overlay[[arr for arr in voxels.T]])\n",
    "    center = voxels[ix]\n",
    "    i,j,k = center\n",
    "    \n",
    "    ## Get MNI coordinates.\n",
    "    x,y,z = apply_affine(affine, center)\n",
    "    \n",
    "    ## Extract max values.\n",
    "    psc = overlay[i,j,k]\n",
    "    F = Fval[i,j,k]\n",
    "    p = pval[i,j,k]\n",
    "    \n",
    "    ## Write to file.\n",
    "    fmni.write('%s,%0.0d,%0.0d,%0.0d,%0.0d,%0.2d,%0.2d,%0.2f,%0.2f,%0.6f\\n' %(roi,i,j,k,x,y,z,psc,F,10.**-p))\n",
    "    \n",
    "    ## Create sphere: find all voxels within extent.\n",
    "    dist = [np.linalg.norm( np.diff( apply_affine(affine,np.vstack([center,v])), axis=0 ) ) for v in voxels]\n",
    "    ix = np.where(np.array(dist)<=extent)\n",
    "    sphere = voxels[ix]\n",
    "    \n",
    "    ## Save.\n",
    "    #hemi, roi = roi.split('-')\n",
    "    #if hemi.startswith('L'): name = '%s-lh' %roi.lower()\n",
    "    #else: name = '%s-rh' %roi.lower()\n",
    "    #np.save(os.path.join(out_dir, name), sphere)\n",
    "    \n",
    "fmni.close()\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-hoc F-statistic Fix\n",
    "Sam realized very late in the game he should have been saving out the pre-TFCE F-statistics. Fortunately these can be recomputed using the WLS code sans TFCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szoro/Documents/software/anaconda2.7/lib/python2.7/site-packages/ipykernel/__main__.py:30: RuntimeWarning: divide by zero encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "space = 'lh'\n",
    "contrast = 'DelibMod'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "def load_sparse_coo(filename):\n",
    "    npz = np.load(filename)\n",
    "    M,N = npz['shape']\n",
    "    return coo_matrix( (npz['data'], (npz['row'],npz['col'])), (M,N) )\n",
    "\n",
    "out_dir = os.path.join('fmri_second_levels', 'FINAL.%s.%s.%s.%s' %(sm,fd,space,contrast))\n",
    "\n",
    "## Load data.\n",
    "npz = np.load(os.path.join(out_dir, 'first_levels.npz'))\n",
    "ces = npz['ces']\n",
    "cesvar = np.abs( 1. / npz['cesvar'] )\n",
    "\n",
    "## Define indices.\n",
    "connectivity = load_sparse_coo(os.path.join('fmri_second_levels', '%s_connectivity.npz' %space))\n",
    "index,  = np.where(~np.isinf(cesvar).sum(axis=1).astype(bool))\n",
    "include = ~np.isinf(cesvar).sum(axis=1).astype(bool)\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup for permutation testing.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load subject information.\n",
    "info = read_csv(os.path.join('demographics.csv'))\n",
    "info = info[~info.Exlude].reset_index()\n",
    "n_subj, _ = info.shape\n",
    "\n",
    "## Build Design Matrix.\n",
    "X = np.zeros((n_subj,2))\n",
    "X[:,0] = 1                                        # Intercept\n",
    "X[:,1] = np.where(info.Scanner == 'Trio', 0, 1)   # Scanner\n",
    "n_subj, n_pred = X.shape\n",
    "\n",
    "sign_flips = np.ones((1,n_subj))\n",
    "n_shuffles = sign_flips.shape[0]\n",
    "\n",
    "## Preallocate arrays for results.\n",
    "shape = [n_shuffles] + list(ces.shape[:-1])\n",
    "Bmap = np.zeros(shape)\n",
    "Fmap = np.zeros(shape)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "'''\n",
    "Following the instructions of Winkler et al. (2014), we use the Freedman and Lane (1983)\n",
    "permutation procedure. This allows us to precompute a number of values ahead of time.\n",
    "\n",
    "To understand the WLS computations, please see:\n",
    "https://github.com/statsmodels/statsmodels/blob/master/statsmodels/base/model.py\n",
    "https://github.com/statsmodels/statsmodels/blob/master/statsmodels/regression/linear_model.py\n",
    "'''\n",
    "\n",
    "def wls(X,Y,W):\n",
    "    B = np.linalg.inv(X.T.dot(W).dot(X)).dot(X.T).dot(W).dot(Y)\n",
    "    ssr = W.dot( np.power(Y - np.dot(X,B),2) ).sum()\n",
    "    scale = ssr / (n_subj - n_pred)\n",
    "    cov_p = np.linalg.inv(X.T.dot(W).dot(X)) * scale\n",
    "    F = np.power(B[0],2) * np.power(cov_p[0,0],-1)\n",
    "    return B[0], F\n",
    "\n",
    "## Loop it!\n",
    "for n, sf in enumerate(sign_flips):\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute statistics.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    for m in index:\n",
    "\n",
    "        ## Update variables.\n",
    "        W = np.diag(cesvar[m])\n",
    "        Y = ces[m]\n",
    "        \n",
    "        ## Permute values.\n",
    "        ## See Winkler et al. (2014), pg. 385\n",
    "        ## To compute Hat Matrix, see: https://en.wikipedia.org/wiki/Projection_matrix and \n",
    "        Z = X[:,1:]\n",
    "        ZZ = Z.dot( np.linalg.inv( Z.T.dot(W).dot(Z) ) ).dot(Z.T).dot(W)\n",
    "        Rz = np.identity(n_subj) - ZZ\n",
    "        Y = np.diag(sf).dot(Rz).dot(Y)\n",
    "        \n",
    "        ## Perform WLS.\n",
    "        Bmap[n,m], Fmap[n,m] = wls(X,Y,W) \n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Save results.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#  \n",
    "\n",
    "def prepare_image(arr, space):\n",
    "    npz = np.load('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "    image = np.zeros_like(npz['mapping'], dtype=float)\n",
    "    \n",
    "    if not space == 'mni305': \n",
    "        image[npz['vertices']] += arr\n",
    "    else:\n",
    "        x,y,z = npz['voxels'].T\n",
    "        image[x,y,z] += arr\n",
    "    \n",
    "    for _ in range(4 - len(image.shape)): image = np.expand_dims(image,-1)\n",
    "    return image\n",
    "\n",
    "## Translate array back into proper space.\n",
    "image = prepare_image(Fmap.squeeze(), space).squeeze()\n",
    "\n",
    "## Load in results table.\n",
    "if space == 'mni305':\n",
    "    results = read_csv('fmri_second_levels/labels/seeds_%s/%s_volume_mni2.csv' %(contrast,contrast))\n",
    "    fscores = [image[i,j,k] for i,j,k in results[['cX','cY','cZ']].as_matrix()]\n",
    "    results['Fpre'] = fscores\n",
    "    results.to_csv('fmri_second_levels/labels/seeds_%s/%s_volume_mni2.csv' %(contrast,contrast))\n",
    "else:\n",
    "    results = read_csv('fmri_second_levels/labels/seeds_%s/%s_surface_mni2.csv' %(contrast,contrast))\n",
    "    if not 'Fpre' in results.columns: results['Fpre'] = np.nan\n",
    "    vertices = results.loc[[True if label.endswith(space) else False for label in results.Label],'V'].as_matrix()\n",
    "    for v in vertices: results.loc[results.V==v,'Fpre'] = image[v]\n",
    "    results.to_csv('fmri_second_levels/labels/seeds_%s/%s_surface_mni2.csv' %(contrast,contrast), index=False)\n",
    "    \n",
    "print 'Done.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "718px",
    "left": "0px",
    "right": "1089px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "toc_position": {
   "height": "767px",
   "left": "0px",
   "right": "1317px",
   "top": "106px",
   "width": "205px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
