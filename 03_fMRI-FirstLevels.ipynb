{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DARPA-ARC Notebook 3: fMRI First Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Subject Task Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "from pandas import read_csv\n",
    "from scipy.special import gammaln\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "mri_dir = 'fmri_first_levels'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Results file.\n",
    "results_file = 'arc_hierarchical_add_FINAL2_regressors'\n",
    "\n",
    "## Define contrasts.\n",
    "conditions = ['Delib','DelibMod','Antcp','AntcpMod','Shock']\n",
    "n_conditions = len(conditions)\n",
    "\n",
    "## Timing information.\n",
    "n_acq = 977\n",
    "tr = 1.75\n",
    "sfreq = 1e2\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define useful functions.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "def spm_hrf(RT, P=None, fMRI_T=16):\n",
    "    p = np.array([6, 16, 1, 1, 6, 0, 32], dtype=float)\n",
    "    if P is not None:\n",
    "        p[0:len(P)] = P\n",
    "\n",
    "    _spm_Gpdf = lambda x, h, l: np.exp(h * np.log(l) + (h - 1) * np.log(x) - (l * x) - gammaln(h))\n",
    "    # modelled hemodynamic response function - {mixture of Gammas}\n",
    "    dt = RT / float(fMRI_T)\n",
    "    u = np.arange(0, int(p[6] / dt + 1)) - p[5] / dt\n",
    "    with np.errstate(divide='ignore'):  # Known division-by-zero\n",
    "        hrf = _spm_Gpdf(u, p[0] / p[2], dt / p[2]) - _spm_Gpdf(u, p[1] / p[3],\n",
    "                                                               dt / p[3]) / p[4]\n",
    "    idx = np.arange(0, int((p[6] / RT) + 1)) * fMRI_T\n",
    "    hrf = hrf[idx]\n",
    "    hrf = hrf / np.sum(hrf)\n",
    "    return hrf\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "\n",
    "results_file = 'stan_results/%s.csv' %results_file\n",
    "df = read_csv(results_file)\n",
    "\n",
    "for subject in np.unique(df.Subject):\n",
    "    \n",
    "    print subject,\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Initialize regressors.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    \n",
    "    ## Setup timing information.\n",
    "    total_time = n_acq * tr\n",
    "    times = np.arange(0, total_time+1./sfreq, 1./sfreq)\n",
    "    n_times = times.shape[0]\n",
    "\n",
    "    ## Initialize boxcars.\n",
    "    neural_signal = np.zeros((n_conditions,n_times))\n",
    "    \n",
    "    ## Extract information.\n",
    "    extract_cols = ['ddb','RiskType','ResponseType','RiskOnset','RT','ShockOnset']\n",
    "    extract_cols = df.loc[df.Subject==subject, extract_cols].copy().as_matrix()\n",
    "    DDB, Risk, Choice, TrialOnset, RT, ShockOnset = extract_cols.T.round(int(np.log10(sfreq)))\n",
    "    \n",
    "    ## Prepare information.\n",
    "    RT += 0.5                     # Reaction time does not factor 0.5s of risk presentation.\n",
    "    RT = np.where(np.isnan(RT), 3.5, RT)\n",
    "    DDB = np.where(np.isnan(DDB),0,DDB)\n",
    "    Risk = np.where(Risk<2,0.1,np.where(Risk<3,0.5,0.9))\n",
    "    Choice = np.where(np.isnan(Choice),0,Choice)\n",
    "    ShockOnset = ShockOnset[~np.isnan(ShockOnset)]\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Generate decision-making boxcars.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "\n",
    "    for onset, duration, parmod in zip(TrialOnset, RT, DDB): \n",
    "        mask = (times >= onset) & (times <= onset + duration)\n",
    "        neural_signal[0,mask] += 1         # Deliberation (intercept)\n",
    "        neural_signal[1,mask] += parmod    # Deliberation (DDB)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Generate expectation boxcars.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    \n",
    "    ## Add anticipation information.\n",
    "    antcp_onset  = TrialOnset + RT\n",
    "    antcp_offset = TrialOnset + 3.5 + 1.25\n",
    "    \n",
    "    for onset, offset, choice, risk in zip(antcp_onset, antcp_offset, Choice, Risk):\n",
    "        mask = (times >= onset) & (times <= offset)\n",
    "        neural_signal[2,mask] += 1\n",
    "        neural_signal[3,mask] += choice * risk\n",
    "    \n",
    "    ## Add shock information.\n",
    "    for onset in ShockOnset:\n",
    "        mask = (times >= onset) & (times <= onset + 0.5)\n",
    "        neural_signal[-1,mask] += 1\n",
    "\n",
    "    ## Perform convolution.\n",
    "    hrf = spm_hrf(1./sfreq)\n",
    "    bold_signal = np.apply_along_axis(np.convolve, 1, neural_signal, v=hrf)\n",
    "    bold_signal = bold_signal[:,:neural_signal.shape[-1]] # Set back to original length.\n",
    "    \n",
    "    ## Downsample to start of TR.\n",
    "    tr_onsets = np.insert( np.cumsum( np.ones(n_acq-1)*tr ), 0, 0 )\n",
    "    ds = np.in1d(times, tr_onsets)\n",
    "    if not ds.sum() == n_acq: raise ValueError('Oh noes!')\n",
    "    bold_signal = bold_signal[:,ds]\n",
    "    \n",
    "    ## Normalize regressors (max height=1).\n",
    "    bold_signal = (bold_signal.T / bold_signal.max(axis=1)).T\n",
    "\n",
    "    ## Save task regressors.\n",
    "    for arr, label in zip(bold_signal, conditions):\n",
    "\n",
    "        f = '%s/%s/arc_001/001/FINAL2.%s.par' %(mri_dir,subject,label)\n",
    "        try: np.savetxt(f, arr[:,np.newaxis], fmt='%s')\n",
    "        except IOError: pass\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute and plot VIF.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    \n",
    "    ## Compute VIF.\n",
    "    bold_signal = bold_signal.T\n",
    "    vif = np.array([variance_inflation_factor(bold_signal,i) for i in range(n_conditions)])\n",
    "    if np.any(np.isinf(vif)): raise ValueError('Oh noes! Check VIF!')\n",
    "    \n",
    "    ## Open figure.\n",
    "    fig = plt.figure(figsize=(18,6))\n",
    "    colors = ['#377eb8','#4daf4a','#e41a1c','#984ea3','#ff7f00','#386cb0','#e7298a','#66a61e']\n",
    "\n",
    "    ## Plot VIF.\n",
    "    ax = plt.subplot2grid((2,3),(0,0),rowspan=2)\n",
    "    ax.bar(np.arange(n_conditions), vif, 0.9, color='#7ec0ee')\n",
    "    ax.set_xlim(-0.1)\n",
    "    ax.set_xticks(np.arange(n_conditions)+0.45)\n",
    "    ax.set_xticklabels(conditions)\n",
    "    ax.set_ylim(0,10)\n",
    "    ax.set_ylabel('VIF', fontsize=20)\n",
    "    ax.set_title('%s Collinearity' %subject.upper(), fontsize=24)\n",
    "\n",
    "    ## Plot decision-making regressors.\n",
    "    ax = plt.subplot2grid((2,3),(0,1),colspan=2)\n",
    "    for arr, label, color in zip(bold_signal.T[:2], conditions[:2], colors[:2]):\n",
    "        ax.plot(tr_onsets, arr, linewidth=2, color=color, alpha=0.8, label=label)\n",
    "    ax.legend(loc=2, bbox_to_anchor=(1.0,0.7), frameon=False, borderpad=0, handletextpad=0.1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xlim(0,180)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('Decision Making', fontsize=24)\n",
    "\n",
    "    ## Plot anticipation regressors.\n",
    "    ax = plt.subplot2grid((2,3),(1,1),colspan=2)\n",
    "    for arr, label, color in zip(bold_signal.T[2:], conditions[2:], colors[2:]):\n",
    "        ax.plot(tr_onsets, arr, linewidth=2, color=color, alpha=0.8, label=label)\n",
    "    ax.legend(loc=2, bbox_to_anchor=(1.0,0.8), frameon=False, borderpad=0, handletextpad=0.1)\n",
    "    ax.set_xlim(0,180)\n",
    "    ax.set_xlabel('Time (s)', fontsize=16)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('Anticipation', fontsize=24)\n",
    "\n",
    "    plt.subplots_adjust(left=0.05, wspace=0.05, hspace=0.3)\n",
    "    plt.savefig('plots/vif/%sreg2_%s.png' %(n_conditions,subject))\n",
    "    plt.close('all')\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Subject Timepoint Censors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from scipy.signal import detrend\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "mri_dir = 'fmri_first_levels'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Timing information.\n",
    "n_acq = 977\n",
    "tr = 1.75\n",
    "\n",
    "## Scrubbing parameters.\n",
    "thresholds = [0.0, 0.5, 0.7, 0.9, 1.1, 1.3]\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define TR onsets.\n",
    "tr_onsets = np.insert( np.cumsum( np.ones(n_acq - 1) * tr ), 0, 0 )\n",
    "\n",
    "## Get subjects list.\n",
    "info = read_csv('demographics.csv')\n",
    "subjects = info.loc[~info.Exlude, 'Subject'].as_matrix()\n",
    "info = open('fmri_second_levels/nuisance_info.csv','w')\n",
    "info.write('Subject,n_mc,FD=0.0,FD=0.5,FD=0.7,FD=0.9,FD=1.1,FD=1.3\\n')\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    info.write('%s,' %subject)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute framewise displacement.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Read motion data.\n",
    "    mc = os.path.join(mri_dir, subject, 'arc_001', '001', 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:7]\n",
    "\n",
    "    ## Invert angular displacement.\n",
    "    fd = mc.copy()\n",
    "    fd[:,:3] = np.deg2rad(fd[:,:3]) \n",
    "    fd[:,:3] *= 50\n",
    "\n",
    "    ## Compute framewise displacement (See Power 2012, 2014).\n",
    "    fd = np.insert( np.abs( np.diff(fd, axis=0) ).sum(axis=1), 0, 0 )\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute motion regressors.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Remove trends.\n",
    "    mc = detrend(mc, axis=0, type='constant')\n",
    "    mc = detrend(mc, axis=0, type='linear')\n",
    "    \n",
    "    ## Perform PCA.\n",
    "    pca = PCA(n_components=6)\n",
    "    mc = pca.fit_transform(mc)\n",
    "    \n",
    "    ## Take only the number of components explaining 90% of the variance.\n",
    "    varexp = np.cumsum(pca.explained_variance_ratio_)\n",
    "    n_components = np.argmax(varexp >= 0.9) + 1\n",
    "    mc = mc[:,:n_components]\n",
    "    \n",
    "    ## Save motion regressor.\n",
    "    f = '%s/%s/arc_001/001/FINAL2.mc.par' %(mri_dir,subject)\n",
    "    np.savetxt(f, mc, fmt='%s')\n",
    "    info.write('%s,' %n_components)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Write scrubbers.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        \n",
    "        ## Find threshold violations.\n",
    "        if not threshold: ix, = np.where(fd >= np.inf)\n",
    "        else: ix, = np.where(fd >= threshold)\n",
    "                \n",
    "        ## Save.\n",
    "        info.write('%s,' %len(ix))\n",
    "        f = '%s/%s/arc_001/001/FINAL2.censor.%s.par' %(mri_dir,subject,threshold)\n",
    "        if len(ix): np.savetxt(f, tr_onsets[ix,np.newaxis], fmt='%s')\n",
    "\n",
    "    info.write('\\n')\n",
    "\n",
    "info.close()\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Censor Analysis: Precompute F maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pandas import read_csv\n",
    "mri_dir = 'fmri_first_levels/concat-sess/FINAL2'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "sm = 6\n",
    "thresholds = [0.0,0.5,0.7,0.9,1.1,1.3]\n",
    "spaces = ['lh','rh','mni305']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup for WLS.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load subject information.\n",
    "info = read_csv('demographics.csv')\n",
    "info = info[~info.Exlude].reset_index()\n",
    "n_subj, _ = info.shape\n",
    "\n",
    "## Build Design Matrix.\n",
    "X = np.zeros((n_subj,2))\n",
    "X[:,0] = 1                                        # Intercept\n",
    "X[:,1] = np.where(info.Scanner == 'Trio', 0, 1)   # Scanner\n",
    "n_subj, n_pred = X.shape\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "def wls(X,Y,W):\n",
    "    B = np.linalg.inv(X.T.dot(W).dot(X)).dot(X.T).dot(W).dot(Y)\n",
    "    ssr = W.dot( np.power(Y - np.dot(X,B),2) ).sum()\n",
    "    scale = ssr / (n_subj - n_pred)\n",
    "    cov_p = np.linalg.inv(X.T.dot(W).dot(X)) * scale\n",
    "    F = np.power(B[0],2) * np.power(cov_p[0,0],-1)\n",
    "    return B[0], F\n",
    "\n",
    "for space in spaces:\n",
    "    \n",
    "    for fd in thresholds:\n",
    "\n",
    "        print space, fd\n",
    "        \n",
    "        results_dir = os.path.join(mri_dir, 'FINAL2.%s.%s.%s' %(sm,fd,space))\n",
    "        \n",
    "        ## Load data.\n",
    "        ces = nib.load(os.path.join(results_dir, 'FINAL2.Delib.par', 'ces.nii.gz')).get_data().squeeze()\n",
    "        cesvar = nib.load(os.path.join(results_dir, 'FINAL2.Delib.par', 'cesvar.nii.gz')).get_data().squeeze()\n",
    "        affine = nib.load(os.path.join(results_dir, 'FINAL2.Delib.par', 'ces.nii.gz')).affine\n",
    "        \n",
    "        ## Reshaping of MNI305 data.\n",
    "        if space == 'mni305':\n",
    "            x,y,z,n_subj = ces.shape\n",
    "            ces = ces.reshape(x*y*z,n_subj)\n",
    "            cesvar = cesvar.reshape(x*y*z,n_subj)\n",
    "        \n",
    "        ## Preallocate arrays for results.\n",
    "        cesvar = np.abs(1./cesvar)\n",
    "        include, = np.where(~np.isinf(cesvar).sum(axis=1).astype(bool))\n",
    "        Fmap = np.repeat(np.nan, ces.shape[0])\n",
    "        \n",
    "        ## Perform WLS.\n",
    "        for i in include:\n",
    "            \n",
    "            ## Update variables\n",
    "            Y = ces[i]\n",
    "            W = np.diag(cesvar[i])\n",
    "            _, Fmap[i] = wls(X,Y,W)\n",
    "        \n",
    "        ## Reshape.\n",
    "        if space == 'mni305': Fmap = Fmap.reshape(x,y,z)\n",
    "        \n",
    "        ## Save.\n",
    "        for _ in range(4 - len(Fmap.shape)): Fmap = np.expand_dims(Fmap, -1)\n",
    "        obj = nib.Nifti1Image(Fmap, affine)\n",
    "        nib.save(obj, os.path.join(results_dir, 'FINAL2.Delib.par', 'F.nii.gz'))\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Censor Analysis: Determine Optimal Threshold\n",
    "Based on the methods from [Siegal et al. (2014)](https://www.ncbi.nlm.nih.gov/pubmed/23861343): *Statistical Improvements in Functional Magnetic Resonance Imaging Analyses Produced by Censoring High-Motion Data Points*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "from mne import read_surface, grow_labels, spatial_tris_connectivity, set_log_level\n",
    "from mne.stats.cluster_level import _find_clusters as find_clusters\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.stats import f as fdist\n",
    "from scipy.ndimage import measurements\n",
    "set_log_level(verbose=False)\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=1.5)\n",
    "%matplotlib inline\n",
    "\n",
    "fs_dir = 'recons'\n",
    "mri_dir = 'fmri_first_levels/concat-sess/FINAL2'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O paramters.\n",
    "sm = 6\n",
    "contrast = 'Delib'\n",
    "censor = True    # {True = Include blobs from all overlays, \n",
    "                 # False = Include blobs from only no-center}\n",
    "\n",
    "## Cluster parameters.\n",
    "cluster_dict = dict(lh = [0.01, 100], rh = [0.01, 100],\n",
    "                    mni305 = [0.01, 20])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define iterators.\n",
    "spaces = ['lh','rh','mni305']\n",
    "thresholds = [0.0, 0.5, 0.7, 0.9, 1.1, 1.3]\n",
    "\n",
    "info = []\n",
    "for n, space in enumerate(spaces):\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    overlays = []\n",
    "    for fd in thresholds: \n",
    "        obj = nib.load(os.path.join(mri_dir, 'FINAL2.%s.%s.%s' %(sm,fd,space), 'FINAL2.%s.par' %contrast, 'F.nii.gz'))\n",
    "        overlays.append( obj.get_data().squeeze() )\n",
    "    overlays = np.array(overlays)\n",
    "\n",
    "    ## Make average map.\n",
    "    if censor: average = overlays.mean(axis=0)\n",
    "    else: average = overlays[0].copy()\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Identify clusters.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    p_value, min_cluster = cluster_dict[space]\n",
    "    min_value = fdist.isf(p_value, 1, 26)\n",
    "    \n",
    "    if space == 'mni305':\n",
    "\n",
    "        masked_average = average > min_value\n",
    "        clusters, n_clusters = measurements.label( masked_average )\n",
    "        clusters = [np.where(clusters==n) for n in np.arange(n_clusters)+1 if (clusters==n).sum() > min_cluster]\n",
    "        \n",
    "    else:\n",
    "\n",
    "        ## Prepare surface information.\n",
    "        _, tris = read_surface(os.path.join(fs_dir, 'fsaverage', 'surf', '%s.white' %space))\n",
    "        connectivity = spatial_tris_connectivity(tris)\n",
    "        include = np.invert(np.isnan(average).astype(bool))\n",
    "        \n",
    "        ## Identify clusters (clusters already sorted by size).\n",
    "        clusters, _ = find_clusters(average, min_value, tail=1, connectivity=connectivity, include=include)\n",
    "        clusters = [c for c in clusters if len(c) > min_cluster]\n",
    "        \n",
    "    print '%s clusters identified for %s.' %(len(clusters), space)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Average across labels / spheres.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    for i, fd in enumerate(thresholds):\n",
    "                    \n",
    "        for j, c in enumerate(clusters):\n",
    "            \n",
    "            fscore = np.nanmean(overlays[i][c])\n",
    "            info.append([fd,space,j,fscore])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute statistics.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "info = DataFrame(np.array(info), columns=('FD','Space','Label','Fscore'))\n",
    "info['Fscore'] = info.Fscore.astype(float)\n",
    "print f_oneway(*[info.loc[info.FD==fd,'Fscore'].as_matrix() for fd in np.unique(info.FD)])\n",
    "print info.groupby(['FD',]).Fscore.mean()\n",
    "\n",
    "## Plot.\n",
    "g = sns.factorplot('Space', 'Fscore', 'FD', info, kind='bar', ci=None, size=4, aspect=2);\n",
    "g.ax.set_ylim(12,16);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
