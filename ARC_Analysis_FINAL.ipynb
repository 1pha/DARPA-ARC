{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "## Load demographics data.\n",
    "df = read_csv('demographics.csv')\n",
    "\n",
    "## Calculate average age.\n",
    "print df.Age.mean(), df.Age.std()\n",
    "\n",
    "## Tabulate genders before rejection.\n",
    "print df.Gender.value_counts()\n",
    "print df[~df.Exlude].Gender.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble Behavior Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import concat, read_csv\n",
    "csv_dir = 'behavior'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "modality = 'mri'\n",
    "scanner_fix = 2.95\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load and assemble data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define subject list.\n",
    "df = read_csv('demographics.csv')\n",
    "subjects = df.loc[~df.Exlude,'Subject']\n",
    "\n",
    "df = []\n",
    "for subject in subjects:\n",
    "    \n",
    "    f = os.path.join(csv_dir, '%s_arc_%s_ser-1' %(subject,modality))\n",
    "    csv = read_csv(os.path.join(csv_dir,f))\n",
    "\n",
    "    ## Add subject. Remove missing trials.\n",
    "    csv['Subject'] = subject\n",
    "\n",
    "    ## Append.\n",
    "    df.append(csv)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Merge and preprocess.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "## Merge and crop.\n",
    "columns = ['Subject', 'Trial', 'RiskType', 'RewardType', 'Reward', 'ResponseType', \n",
    "           'ResponseTime','RiskOnset','ShockOnset']\n",
    "df = concat(df)[columns].reset_index(drop=True)\n",
    "\n",
    "## Replace missing responses.\n",
    "df.loc[df.ResponseType==99,'ResponseType'] = np.nan\n",
    "\n",
    "## Due to code-scanner issues, we will impute all reaction times greater than 2.95s.\n",
    "if scanner_fix: df.loc[df.ResponseTime>scanner_fix, 'ResponseTime'] = np.nan\n",
    "\n",
    "## Save.\n",
    "df.to_csv('stan_results/arc_%s_FINAL.csv' %modality, index=False)\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Gamma Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "from ipywidgets import interact\n",
    "from scipy.stats import gamma\n",
    "%matplotlib inline\n",
    "\n",
    "def visualize(mu, k):\n",
    "    scale = mu / float(k)\n",
    "    x = np.linspace(0,3,1e3)\n",
    "    y = gamma.pdf(x, k, scale=scale)\n",
    "    plt.plot(x,y)\n",
    "    plt.ylim(0,5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "interact(visualize, mu=(0,3,0.1), k=(0,100,1))\n",
    "\n",
    "x = np.linspace(0,1e2,1e3)\n",
    "y = gamma.pdf(x, 1, scale=1/0.05)\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Models with Stan\n",
    "See [here](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations) for discussion of choice of logistic priors. Namely:\n",
    ">Assuming that nonbinary variables have been scaled to have mean 0 and standard deviation 0.5, [Gelman et al (2008)](https://arxiv.org/pdf/0901.4011.pdf) recommended *student_t(1,0,2.5)*, i.e. Cauchy distribution. Later it has been observed that this has too thick tails, so that in cases where data is not informative (e.g. in case of separation) the sampling from the posterior is challenging (see e.g. [Ghosh et al, 2015](http://arxiv.org/abs/1507.07170)). Thus Student's t distribution with higher degrees of freedom is recommended. There is not yet conclusive results what specific value should be recommended, and thus the current recommendation is to choose 3<nu<7. \n",
    "\n",
    ">Normal distribution is not recommended as a weakly informative prior, because it is not robust (see [O'Hagan (1979)](https://www.jstor.org/stable/2985064)). Normal distribution would be fine as an informative prior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cPickle, os, pystan\n",
    "import numpy as np\n",
    "from pandas import DataFrame, read_csv, get_dummies\n",
    "def zscore(arr): return (arr - np.nanmean(arr)) / np.nanstd(arr) \n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Specify parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "file_name = 'arc_mri_FINAL.csv'\n",
    "model_name = 'arc_non-hierarchical_FINAL'\n",
    "out_name = 'arc_non-hierarchical_add_FINAL2'\n",
    "interactions = False\n",
    "\n",
    "## Model parameters\n",
    "nu = 5\n",
    "\n",
    "## Sampling parameters.\n",
    "chains = 4\n",
    "samples = 2000\n",
    "thin = 4\n",
    "seed = 47404\n",
    "n_jobs = 2\n",
    "\n",
    "print('n_samples: %s' %(chains * samples / 2 / thin))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Assemble data for model.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load data.\n",
    "df = read_csv('stan_results/%s' %file_name)\n",
    "df = df[~np.isnan(df.ResponseType)].reset_index(drop=True)\n",
    "\n",
    "## Make subject index.\n",
    "subjects, ix = np.unique(df.Subject, return_inverse=True)        \n",
    "ix += 1\n",
    "\n",
    "## Make missing data index.\n",
    "mi = df.ResponseTime.isnull().astype(int)\n",
    "mi *= np.cumsum(mi)\n",
    "\n",
    "## Assemble indepedent variables.\n",
    "_, med_risk, high_risk = get_dummies(df.RiskType).as_matrix().T \n",
    "risk = np.vstack([med_risk, high_risk])\n",
    "intercept = np.ones_like(med_risk)\n",
    "reward = df.Reward.as_matrix()                      \n",
    "\n",
    "if interactions: X = np.vstack([intercept,risk,reward,risk*reward]).T \n",
    "else: X = np.vstack([intercept,risk,reward]).T  \n",
    "\n",
    "## Assemble depedent variables.\n",
    "N = df.ResponseType.as_matrix().astype(int)\n",
    "Z = df.ResponseTime.as_matrix()\n",
    "Z[np.isnan(Z)] = 99\n",
    "\n",
    "## Z-score variables.\n",
    "zX = X.copy()\n",
    "if not interactions: \n",
    "    zX[:,-1] = zscore(X[:,-1])\n",
    "else:\n",
    "    zX[:,3] = zscore(zX[:,3])\n",
    "    zX[:,4] = zX[:,1] * zX[:,3]\n",
    "    zX[:,5] = zX[:,2] * zX[:,3]\n",
    "\n",
    "## Assemble metadata.\n",
    "n_obs, n_pred = X.shape\n",
    "n_subj = subjects.shape[0]\n",
    "n_miss = len(mi.nonzero()[0])\n",
    "\n",
    "## Assemble data.\n",
    "data = dict(n_obs=n_obs, n_pred=n_pred, n_subj=n_subj, n_miss=n_miss, ix=ix, mi=mi, X=zX, N=N, Z=Z)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Perform Bayesian modeling.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "print 'Running Stan.',\n",
    "f = 'stan_models/%s.txt' %model_name\n",
    "fit = pystan.stan(file=f, data=data, chains=chains, iter=samples, thin=thin,\n",
    "                  seed=seed, n_jobs=n_jobs)\n",
    "print 'Finished.'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Save summary file.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "summary = fit.summary()\n",
    "summary = DataFrame(summary['summary'], columns=summary['summary_colnames'], index=summary['summary_rownames'])\n",
    "f = 'stan_results/%s.csv' %out_name\n",
    "summary.to_csv(f)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Extract Results.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "results = fit.extract()\n",
    "\n",
    "## Append data to results.\n",
    "results['Subjects'] = subjects\n",
    "results['X'] = X\n",
    "results['zX'] = zX\n",
    "results['N'] = N\n",
    "results['Z'] = Z\n",
    "results['ix'] = ix\n",
    "results['RiskOnset'] = df.RiskOnset.as_matrix()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Save results.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "print 'Saving data.'\n",
    "\n",
    "## Save all data.\n",
    "f = 'stan_results/%s.pickle' %out_name\n",
    "with open(f, 'wb') as f: cPickle.dump(results, f)\n",
    "    \n",
    "## Save log-likelihood for R.\n",
    "np.savetxt('stan_results/%s_loglik.txt' %out_name, np.log(results[u'PointPosteriors']))\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Predictive Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cPickle, os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from pandas import DataFrame\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "results_file = 'arc_non-hierarchical_add_FINAL2'\n",
    "ncol = 4\n",
    "decim = 4\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Open results file.\n",
    "print 'Loading data.'\n",
    "f = 'stan_results/%s.pickle' %results_file\n",
    "with open(f, 'rb') as f: results = cPickle.load(f)\n",
    "\n",
    "## Extract variables.\n",
    "subjects = results['Subjects']\n",
    "ix = results['ix'] - 1\n",
    "n_subjects = ix.max() + 1\n",
    "nrow = int(np.ceil(n_subjects / ncol))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Evaluate fit to choice data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#   \n",
    "\n",
    "print 'Evaluating choice data.'\n",
    "\n",
    "def bernoulli(p): return np.random.binomial(1,p,size=1)\n",
    "Bernoulli = np.vectorize(bernoulli)\n",
    "\n",
    "## Compute true ratio of takes.\n",
    "ratio_obs = np.array([results['N'][ix==i].mean() for i in range(n_subjects)])\n",
    "\n",
    "## Generate simulations of take/no-take.\n",
    "np.random.seed(47404)\n",
    "sN = Bernoulli(results['theta'])\n",
    "\n",
    "## Compute ratio take from simulations.\n",
    "ratio_sim = np.array([sN[:,ix==i].mean(axis=1) for i in range(n_subjects)])\n",
    "\n",
    "## Compute difference between observed and median of distribution.\n",
    "ppc = ratio_obs - np.median(ratio_sim, axis=1)\n",
    "\n",
    "## Compute group statistic.\n",
    "rms_ppc = np.sqrt(np.power(ppc,2).mean())\n",
    "\n",
    "## Initialize plots.\n",
    "fig, axes = plt.subplots(nrow,ncol,figsize=(nrow*3,ncol*4),sharey=True)\n",
    "\n",
    "for n, subject in enumerate(subjects):\n",
    "    \n",
    "    axes[n/ncol,n%ncol].hist(ratio_sim[n], bins=8, color='#7ec0ee')\n",
    "    axes[n/ncol,n%ncol].vlines(ratio_obs[n], 0, 500, linewidth=2, linestyle='--')\n",
    "    axes[n/ncol,n%ncol].set_title(subject.upper())\n",
    "    x1, x2 = axes[n/ncol,n%ncol].get_xlim()\n",
    "    xticks = np.linspace(x1,x2,3).round(2)\n",
    "    axes[n/ncol,n%ncol].set_xticks(xticks)\n",
    "    \n",
    "plt.suptitle('PPC = %0.3f' %rms_ppc, y=0.99)\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig('plots/ppc/%s_ppc_choice.png' %results_file)\n",
    "plt.close('all')\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Evaluate fit to reaction time data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#  \n",
    "\n",
    "print 'Evaluating reaction time data.'\n",
    "\n",
    "## Extract and provide imputations data.\n",
    "Z = results['Z'].copy()\n",
    "Z[np.where(Z==99)] = results['Zm'].mean(axis=0)\n",
    "\n",
    "## Initialize plots.\n",
    "fig, axes = plt.subplots(nrow,ncol,figsize=(nrow*3,ncol*4))\n",
    "\n",
    "for n, subject in enumerate(subjects):\n",
    "\n",
    "    ## Extract info.\n",
    "    a0 = results['a0'][:,n]\n",
    "    if 'non-' in results_file: a1 = results['a1_mu']\n",
    "    else: a1 = results['a1'][:,n]\n",
    "    shape = results['shape'][:,n]\n",
    "    ddb = results['ddb'][:,ix==n]\n",
    "\n",
    "    ## Calculate gamma parameters.\n",
    "    mu = a0 + (a1 * ddb.T) \n",
    "    scale = mu / shape\n",
    "\n",
    "    ## Simulate reaction times.\n",
    "    np.random.seed(47404)\n",
    "    def gamma(shape,scale): return np.random.gamma(shape,scale,size=1)\n",
    "    Gamma = np.vectorize(gamma)\n",
    "    rt_sim = Gamma(shape,scale)\n",
    "\n",
    "    ## Extract observed reaction times.\n",
    "    rt_obs = Z[ix==n]\n",
    "\n",
    "    ## Plot observed.\n",
    "    density, bins = np.histogram(rt_obs, bins=5, density=True)\n",
    "    x = bins[:-1] + np.diff(bins) \n",
    "    axes[n/ncol,n%ncol].plot(x,density,linewidth=3,color='#7ec0ee')\n",
    "    axes[n/ncol,n%ncol].set_title(subject.upper())\n",
    "    axes[n/ncol,n%ncol].set_xlim(0,5)\n",
    "\n",
    "    ## Plot simulated.\n",
    "    for arr in rt_sim.T[::decim]:\n",
    "        density, bins = np.histogram(arr, bins=5, density=True)\n",
    "        x = bins[:-1] + np.diff(bins) \n",
    "        axes[n/ncol,n%ncol].plot(x,density,color='k',alpha=0.01)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig('plots/ppc/%s_ppc_rt.png' %results_file)\n",
    "plt.close('all')\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison: Compute WAIC / CV-LOO\n",
    "Performed in R. See other script.\n",
    "\n",
    "### Assemble model outputs for fMRI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cPickle, os\n",
    "import numpy as np\n",
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "results_file = 'arc_hierarchical_add_FINAL2'\n",
    "csv_file = 'arc_mri_FINAL'\n",
    "out_file = 'arc_hierarchical_add_FINAL2_regressors'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Open results file.\n",
    "print 'Loading data.'\n",
    "f = 'stan_results/%s.pickle' %results_file\n",
    "with open(f, 'rb') as f: results = cPickle.load(f)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Build dataframe.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "print 'Assembling dataframe.'\n",
    "\n",
    "## Initialize.\n",
    "df = dict()\n",
    "\n",
    "## Add subject info.\n",
    "ix = results['ix'] - 1\n",
    "df['Subject'] = results['Subjects'][ix]\n",
    "\n",
    "## Add regressors.\n",
    "df['RiskType'] = np.where(results['X'][:,1]==1, 2, np.where(results['X'][:,2]==1,3,1))\n",
    "df['Reward']   = results['X'][:,3]\n",
    "df['theta']    = np.median(results['theta'], axis=0)\n",
    "df['ddb']      = 0.25 - np.power(df['theta'] - 0.5, 2)\n",
    "\n",
    "## Extract and provide imputations data.\n",
    "Z = results['Z'].copy()\n",
    "Z[np.where(Z==99)] = results['Zm'].mean(axis=0)\n",
    "df['RT'] = Z\n",
    "df['RiskOnset'] = results['RiskOnset']\n",
    "\n",
    "## Assemble dataframe.\n",
    "df = DataFrame(df)\n",
    "\n",
    "## Merge with original file.\n",
    "csv = read_csv('stan_results/%s.csv' %csv_file)\n",
    "csv.drop('ResponseTime', axis=1, inplace=True)\n",
    "\n",
    "df = df.merge(csv, how='outer', on=['Subject','RiskType','Reward','RiskOnset'])\n",
    "df = df.sort_values(['Subject','Trial']).reset_index(drop=True)\n",
    "df = df[['Subject','Trial','RiskType','ResponseType','ddb','RT','RiskOnset','ShockOnset']]\n",
    "\n",
    "## Save.\n",
    "print 'Saving dataframe.'\n",
    "df.to_csv('stan_results/%s.csv' %out_file, index=False)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: MRI Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare White Matter Masks\n",
    "Please see: /space/sophia/2/users/DARPA-Behavior/notebooks/bayes/decision_making/NN_bayes_2016/scripts/wm_masks.csh\n",
    "\n",
    "### Extract and Store Grey/White Matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pandas import read_csv\n",
    "from mne.filter import construct_iir_filter, filter_data\n",
    "def demean(arr): return arr - arr.mean()\n",
    "\n",
    "mri_dir = '/autofs/space/lilli_002/users/DARPA-ARC/'\n",
    "subjects_dir = '/autofs/space/lilli_001/users/DARPA-Recons'\n",
    "out_dir = '/space/sophia/2/users/DARPA-Behavior/notebooks/bayes/decision_making/NN_bayes_2016/motion'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "decim = 250\n",
    "tr = 1.75\n",
    "high_pass = 100 # seconds\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "info = read_csv('demographics.csv')\n",
    "subjects = info.loc[~info.Exlude,'Subject'].as_matrix()\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    print subject,\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and prepare masks.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    brainmask = os.path.join(mri_dir, subject, 'arc_001', '001', 'masks', 'brain.nii.gz')\n",
    "    brainmask = np.where( nib.load(brainmask).get_data(), 1, 0 ) # Binarize the mask\n",
    "\n",
    "    wm = os.path.join(mri_dir, subject, 'arc_001', '001', 'masks', 'wm.mgz')\n",
    "    wm = np.where( nib.load(wm).get_data(), 1, 0 ) # Binarize the mask\n",
    "\n",
    "    gm = brainmask - wm\n",
    "    \n",
    "    ## Reduce to indices of interest.\n",
    "    gm = np.vstack(np.where(gm))[:,::decim]\n",
    "    wm = np.vstack(np.where(wm))[:,::decim]\n",
    "    del brainmask\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and slice through EPI image.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load data.\n",
    "    obj = nib.load(os.path.join(mri_dir, subject, 'arc_001', '001', 'fmcpr.nii'))\n",
    "    _,_,_,n_acq = obj.shape\n",
    "    \n",
    "    ## Preallocoate space for timeseries.\n",
    "    gmts = np.zeros((n_acq, gm.shape[-1]))\n",
    "    wmts = np.zeros((n_acq, wm.shape[-1]))\n",
    "    \n",
    "    for n in range(n_acq):\n",
    "\n",
    "        ## Slice image.\n",
    "        acq = obj.dataobj[..., n]\n",
    "        \n",
    "        ## Store grey matter.\n",
    "        gmts[n] += acq[gm[0],gm[1],gm[2]]\n",
    "        \n",
    "        ## Store white matter.\n",
    "        wmts[n] += acq[wm[0],wm[1],wm[2]]\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Preprocessing data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Construct highpass filter.\n",
    "    sfreq = 1. / tr\n",
    "    high_pass = 1. / high_pass\n",
    "    iir_params = dict(order=2, ftype='butter', output='sos') # Following Power et al. (2014)\n",
    "    iir_params = construct_iir_filter(iir_params, high_pass, None, sfreq, 'highpass', return_copy=False)  \n",
    "    \n",
    "    ## Filter data.\n",
    "    gmts = filter_data(gmts.T, sfreq, high_pass, None, method='iir', iir_params=iir_params, verbose=False)\n",
    "    wmts = filter_data(wmts.T, sfreq, high_pass, None, method='iir', iir_params=iir_params, verbose=False)\n",
    "    \n",
    "    ## De-mean (we'll save this for later.)\n",
    "    #gmts = np.apply_along_axis(demean, 1, gmts)\n",
    "    #wmts = np.apply_along_axis(demean, 1, wmts)\n",
    "    \n",
    "    ## Re-organize (center outwards).\n",
    "    gmts = gmts[ np.argsort( np.power( np.apply_along_axis(demean, 1, gm), 2 ).sum(axis=0) ) ]\n",
    "    wmts = gmts[ np.argsort( np.power( np.apply_along_axis(demean, 1, wm), 2 ).sum(axis=0) ) ]\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Save data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    f = os.path.join(out_dir, '%s_arc_qc_data' %subject)\n",
    "    np.savez_compressed(f, gm=gmts, wm=wmts, iir_params=iir_params )\n",
    "    del gmts, wmts, obj\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Summary Statistics of and Visualize Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from collections import defaultdict\n",
    "from pandas import DataFrame, read_csv\n",
    "def demean(arr): return arr - arr.mean()\n",
    "\n",
    "mri_dir = '/autofs/space/lilli_002/users/DARPA-ARC'\n",
    "behavior_dir = '/space/sophia/2/users/DARPA-Behavior/arc/csv'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "thresholds = [0.5,0.9,1.3]\n",
    "selected_threshold = 0.9\n",
    "tr = 1.75\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "info = read_csv('demographics.csv')\n",
    "subjects = info.loc[~info.Exlude,'Subject'].as_matrix()\n",
    "stats = defaultdict(list)\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and prepare MRI data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load gray/white matter timeseries.\n",
    "    npz = np.load('fmri_motion/%s_arc_qc_data.npz' %subject)\n",
    "    gm = np.apply_along_axis(demean, 1, npz['gm'])\n",
    "    wm = np.apply_along_axis(demean, 1, npz['wm'])\n",
    "    \n",
    "    ''' A NOTE ON MOTION DATA.\n",
    "    1) Understanding Freesurfer motion outputs: https://mail.nmr.mgh.harvard.edu/pipermail//freesurfer/2013-May/030273.html\n",
    "    2) Understanding angular displacement: https://en.wikipedia.org/wiki/Angular_displacement\n",
    "    3) Understanding framewise displacement: See Power 2012, 2014\n",
    "    '''\n",
    "\n",
    "    ## Read motion data.\n",
    "    mc = os.path.join(mri_dir, subject, 'arc_001', '001', 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:7]\n",
    "\n",
    "    ## Invert angular displacement.\n",
    "    mc[:,:3] = np.deg2rad(mc[:,:3]) # Convert degrees to radians\n",
    "    mc[:,:3] *= 50                  # Convert radians to mm [Following Power 2012, we assume a head ~ sphere w/ r=50mm]\n",
    "\n",
    "    ## Compute framewise displacement (See Power 2012, 2014).\n",
    "    fd = np.insert( np.abs( np.diff(mc, axis=0) ).sum(axis=1), 0, 0 )\n",
    "\n",
    "    ## Compute absolute displacement. \n",
    "    rot = ( np.abs( mc - mc[0] )[:,:3] ).sum(axis=1)\n",
    "    trans = ( np.abs( mc - mc[0] )[:,3:] ).sum(axis=1)\n",
    "    \n",
    "    ## Compute volumes to remove.\n",
    "    rejections = np.zeros_like(thresholds)\n",
    "    for n, threshold in enumerate(thresholds): rejections[n] += (fd >= threshold).sum()\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and prepare behavior data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Load behavior CSV.\n",
    "    df = read_csv(os.path.join(behavior_dir, '%s_arc_mri_ser-1' %subject))\n",
    "    \n",
    "    ## Create onsets of each TR.\n",
    "    onsets = np.cumsum( np.ones_like(fd) * tr )\n",
    "    onsets = np.insert(onsets, 0, 0)\n",
    "\n",
    "    ## Define onsets/offsets of trials & shocks.\n",
    "    trial_starts = df.RiskOnset.as_matrix()\n",
    "    trial_ends = np.append(df.FixOnset[1:], trial_starts[-1] + 5.25)\n",
    "    shock_starts = df.ShockOnset[~df.ShockOnset.isnull()].as_matrix()\n",
    "    shock_ends = np.array([trial_ends[np.argmin((trial_ends - s)**2)] for s in shock_starts])\n",
    "\n",
    "    ## Digitize onsets/offsets.\n",
    "    trial_starts = np.digitize(trial_starts, onsets)\n",
    "    trial_ends = np.digitize(trial_ends, onsets)\n",
    "    shock_starts = np.digitize(shock_starts, onsets)\n",
    "    shock_ends = np.digitize(shock_ends, onsets)\n",
    "    \n",
    "    ## Make boxcars for plotting.\n",
    "    trials = np.zeros_like(fd)\n",
    "    for i,j in zip(trial_starts,trial_ends): trials[i:j+1] += 1 \n",
    "        \n",
    "    shocks = np.zeros_like(fd)\n",
    "    for i,j in zip(shock_starts,shock_ends): shocks[i:j+1] += 1 \n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Calculate summary statistics.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    stats['Subject'] += [subject]\n",
    "    \n",
    "    ## Add motion information.\n",
    "    stats['Abs_Disp_Rot']   += [rot.max()]\n",
    "    stats['Abs_Disp_Trans'] += [trans.max()]\n",
    "    stats['FD_mean'] += [fd.mean()]\n",
    "    stats['FD_sd']   += [fd.std()]\n",
    "    stats['FD_max']  += [fd.max()]\n",
    "    \n",
    "    ## Calculate number of rejections.\n",
    "    fd_index, = np.where(fd >= selected_threshold)\n",
    "    n_reject = len(fd_index)\n",
    "    stats['FD_reject'] += [n_reject]\n",
    "    \n",
    "    ## Calculate fraction of rejected displacements across all \n",
    "    ## instances of a portion of the run.\n",
    "    stats['FD_frac_task']  += [((trials) * (fd >= selected_threshold)).mean()]\n",
    "    stats['FD_frac_rest']  += [((1 - trials) * (fd >= selected_threshold)).mean()]\n",
    "    stats['FD_frac_shock'] += [((shocks) * (fd >= selected_threshold)).mean()]\n",
    "    \n",
    "    ## Calculate percentages of rejection displacesments\n",
    "    ## within given categories (non-shock task, shock, rest)\n",
    "    stats['FD_perc_rest']    += [(1-trials)[fd_index].sum() / float(n_reject) ]\n",
    "    stats['FD_perc_shock']   += [shocks[fd_index].sum() / float(n_reject) ]\n",
    "    stats['FD_perc_task_ns'] += [1 - stats['FD_perc_rest'][-1] - stats['FD_perc_shock'][-1]]\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plotting.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Initialize figure.\n",
    "    fig = plt.figure(figsize=(18,9), dpi=120)\n",
    "    nrow = 11\n",
    "    \n",
    "    ## Plot absolute motion.\n",
    "    ax = plt.subplot2grid((nrow,1),(0,0)) \n",
    "    ax.plot(rot, linewidth=2, label='Rot', color='#4daf4a')\n",
    "    ax.plot(trans, linewidth=2, label='Trans', color='#984ea3')\n",
    "    ax.legend(loc=2, frameon=False, borderpad=0, labelspacing=0.1, handletextpad=0.1)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel('Dist (mm)', fontsize=10)\n",
    "    ax.set_ylim(0,10)\n",
    "    ax.set_yticks([0,5,10])\n",
    "    ax.tick_params(axis='y', which='major', labelsize=6)\n",
    "    ax.set_title('%s ARC Motion' %subject.upper(), fontsize=24)\n",
    "    \n",
    "    ## Plot shocks.\n",
    "    ax = plt.subplot2grid((nrow,1),(1,0))\n",
    "    ax.plot(shocks*0.5, linewidth=2, color='k')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('Shocks\\n', fontsize=10)\n",
    "    \n",
    "    ## Plot framewise displacement.\n",
    "    ax = plt.subplot2grid((nrow,1),(2,0),rowspan=2)\n",
    "    ax.plot(fd, linewidth=2, color='k')\n",
    "    for t, r, c in zip(thresholds, rejections, ['#e41a1c', '#377eb8','#4daf4a']): \n",
    "        ax.plot(np.ones_like(fd)*t, linewidth=2, linestyle='--', alpha=0.9, label='%s (%s volumes)' %(t,int(r)), color=c)\n",
    "    ax.legend(loc=2, frameon=False, borderpad=0, labelspacing=0.1, handletextpad=0.1)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel('Dist (mm)', fontsize=11)\n",
    "    ax.set_ylim(0,2.5)\n",
    "    ax.tick_params(axis='y', which='major', labelsize=8)\n",
    "    \n",
    "    ## Plot gray matter.\n",
    "    ax = plt.subplot2grid((nrow,1),(4,0),rowspan=4)\n",
    "    cbar = ax.imshow(gm, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('Gray Matter', fontsize=14)\n",
    "\n",
    "    ## Plot white matter.\n",
    "    ax = plt.subplot2grid((nrow,1),(8,0),rowspan=3)\n",
    "    cbar = ax.imshow(wm, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('White Matter', fontsize=14)\n",
    "    ax.set_xlabel('Acquisitions', fontsize=16)\n",
    "\n",
    "    plt.subplots_adjust(left=0.05, right=0.975, top=0.95, bottom=0.06, hspace=0.1)\n",
    "    plt.savefig('plots/motion/%s_motion_raw.png' %subject)\n",
    "    #plt.show()\n",
    "    plt.close('all')\n",
    "    \n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Save summary statistics.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "stats = DataFrame(stats)\n",
    "stats = stats[['Subject', 'Abs_Disp_Rot', 'Abs_Disp_Trans', 'FD_mean', 'FD_sd', 'FD_max',\n",
    "               'FD_reject', 'FD_frac_task', 'FD_frac_shock', 'FD_frac_rest',\n",
    "               'FD_perc_task_ns',  'FD_perc_shock', 'FD_perc_rest']]\n",
    "stats.to_csv('fmri_motion/motion_stats.csv', index=False)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rejections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Present Motion Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "df = read_csv('motion/motion_stats.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: fMRI Analysis (First Levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Subject Task Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "from pandas import read_csv\n",
    "from scipy.special import gammaln\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "mri_dir = 'fmri_first_levels'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Results file.\n",
    "results_file = 'arc_hierarchical_add_FINAL2_regressors'\n",
    "\n",
    "## Define contrasts.\n",
    "conditions = ['Delib','DelibMod','Antcp','AntcpMod','Shock']\n",
    "n_conditions = len(conditions)\n",
    "\n",
    "## Timing information.\n",
    "n_acq = 977\n",
    "tr = 1.75\n",
    "sfreq = 1e2\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define useful functions.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "def spm_hrf(RT, P=None, fMRI_T=16):\n",
    "    p = np.array([6, 16, 1, 1, 6, 0, 32], dtype=float)\n",
    "    if P is not None:\n",
    "        p[0:len(P)] = P\n",
    "\n",
    "    _spm_Gpdf = lambda x, h, l: np.exp(h * np.log(l) + (h - 1) * np.log(x) - (l * x) - gammaln(h))\n",
    "    # modelled hemodynamic response function - {mixture of Gammas}\n",
    "    dt = RT / float(fMRI_T)\n",
    "    u = np.arange(0, int(p[6] / dt + 1)) - p[5] / dt\n",
    "    with np.errstate(divide='ignore'):  # Known division-by-zero\n",
    "        hrf = _spm_Gpdf(u, p[0] / p[2], dt / p[2]) - _spm_Gpdf(u, p[1] / p[3],\n",
    "                                                               dt / p[3]) / p[4]\n",
    "    idx = np.arange(0, int((p[6] / RT) + 1)) * fMRI_T\n",
    "    hrf = hrf[idx]\n",
    "    hrf = hrf / np.sum(hrf)\n",
    "    return hrf\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "\n",
    "results_file = 'stan_results/%s.csv' %results_file\n",
    "df = read_csv(results_file)\n",
    "\n",
    "for subject in np.unique(df.Subject):\n",
    "    \n",
    "    print subject,\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Initialize regressors.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    \n",
    "    ## Setup timing information.\n",
    "    total_time = n_acq * tr\n",
    "    times = np.arange(0, total_time+1./sfreq, 1./sfreq)\n",
    "    n_times = times.shape[0]\n",
    "\n",
    "    ## Initialize boxcars.\n",
    "    neural_signal = np.zeros((n_conditions,n_times))\n",
    "    \n",
    "    ## Extract information.\n",
    "    extract_cols = ['ddb','RiskType','ResponseType','RiskOnset','RT','ShockOnset']\n",
    "    extract_cols = df.loc[df.Subject==subject, extract_cols].copy().as_matrix()\n",
    "    DDB, Risk, Choice, TrialOnset, RT, ShockOnset = extract_cols.T.round(int(np.log10(sfreq)))\n",
    "    \n",
    "    ## Prepare information.\n",
    "    RT += 0.5                     # Reaction time does not factor 0.5s of risk presentation.\n",
    "    RT = np.where(np.isnan(RT), 3.5, RT)\n",
    "    DDB = np.where(np.isnan(DDB),0,DDB)\n",
    "    Risk = np.where(Risk<2,0.1,np.where(Risk<3,0.5,0.9))\n",
    "    Choice = np.where(np.isnan(Choice),0,Choice)\n",
    "    ShockOnset = ShockOnset[~np.isnan(ShockOnset)]\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Generate decision-making boxcars.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "\n",
    "    for onset, duration, parmod in zip(TrialOnset, RT, DDB): \n",
    "        mask = (times >= onset) & (times <= onset + duration)\n",
    "        neural_signal[0,mask] += 1         # Deliberation (intercept)\n",
    "        neural_signal[1,mask] += parmod    # Deliberation (DDB)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Generate expectation boxcars.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    \n",
    "    ## Add anticipation information.\n",
    "    antcp_onset  = TrialOnset + RT\n",
    "    antcp_offset = TrialOnset + 3.5 + 1.25\n",
    "    \n",
    "    for onset, offset, choice, risk in zip(antcp_onset, antcp_offset, Choice, Risk):\n",
    "        mask = (times >= onset) & (times <= offset)\n",
    "        neural_signal[2,mask] += 1\n",
    "        neural_signal[3,mask] += choice * risk\n",
    "    \n",
    "    ## Add shock information.\n",
    "    for onset in ShockOnset:\n",
    "        mask = (times >= onset) & (times <= onset + 0.5)\n",
    "        neural_signal[-1,mask] += 1\n",
    "\n",
    "    ## Perform convolution.\n",
    "    hrf = spm_hrf(1./sfreq)\n",
    "    bold_signal = np.apply_along_axis(np.convolve, 1, neural_signal, v=hrf)\n",
    "    bold_signal = bold_signal[:,:neural_signal.shape[-1]] # Set back to original length.\n",
    "    \n",
    "    ## Downsample to start of TR.\n",
    "    tr_onsets = np.insert( np.cumsum( np.ones(n_acq-1)*tr ), 0, 0 )\n",
    "    ds = np.in1d(times, tr_onsets)\n",
    "    if not ds.sum() == n_acq: raise ValueError('Oh noes!')\n",
    "    bold_signal = bold_signal[:,ds]\n",
    "    \n",
    "    ## Normalize regressors (max height=1).\n",
    "    bold_signal = (bold_signal.T / bold_signal.max(axis=1)).T\n",
    "\n",
    "    ## Save task regressors.\n",
    "    for arr, label in zip(bold_signal, conditions):\n",
    "\n",
    "        f = '%s/%s/arc_001/001/FINAL2.%s.par' %(mri_dir,subject,label)\n",
    "        try: np.savetxt(f, arr[:,np.newaxis], fmt='%s')\n",
    "        except IOError: pass\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute and plot VIF.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    \n",
    "    ## Compute VIF.\n",
    "    bold_signal = bold_signal.T\n",
    "    vif = np.array([variance_inflation_factor(bold_signal,i) for i in range(n_conditions)])\n",
    "    if np.any(np.isinf(vif)): raise ValueError('Oh noes! Check VIF!')\n",
    "    \n",
    "    ## Open figure.\n",
    "    fig = plt.figure(figsize=(18,6))\n",
    "    colors = ['#377eb8','#4daf4a','#e41a1c','#984ea3','#ff7f00','#386cb0','#e7298a','#66a61e']\n",
    "\n",
    "    ## Plot VIF.\n",
    "    ax = plt.subplot2grid((2,3),(0,0),rowspan=2)\n",
    "    ax.bar(np.arange(n_conditions), vif, 0.9, color='#7ec0ee')\n",
    "    ax.set_xlim(-0.1)\n",
    "    ax.set_xticks(np.arange(n_conditions)+0.45)\n",
    "    ax.set_xticklabels(conditions)\n",
    "    ax.set_ylim(0,10)\n",
    "    ax.set_ylabel('VIF', fontsize=20)\n",
    "    ax.set_title('%s Collinearity' %subject.upper(), fontsize=24)\n",
    "\n",
    "    ## Plot decision-making regressors.\n",
    "    ax = plt.subplot2grid((2,3),(0,1),colspan=2)\n",
    "    for arr, label, color in zip(bold_signal.T[:2], conditions[:2], colors[:2]):\n",
    "        ax.plot(tr_onsets, arr, linewidth=2, color=color, alpha=0.8, label=label)\n",
    "    ax.legend(loc=2, bbox_to_anchor=(1.0,0.7), frameon=False, borderpad=0, handletextpad=0.1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xlim(0,180)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('Decision Making', fontsize=24)\n",
    "\n",
    "    ## Plot anticipation regressors.\n",
    "    ax = plt.subplot2grid((2,3),(1,1),colspan=2)\n",
    "    for arr, label, color in zip(bold_signal.T[2:], conditions[2:], colors[2:]):\n",
    "        ax.plot(tr_onsets, arr, linewidth=2, color=color, alpha=0.8, label=label)\n",
    "    ax.legend(loc=2, bbox_to_anchor=(1.0,0.8), frameon=False, borderpad=0, handletextpad=0.1)\n",
    "    ax.set_xlim(0,180)\n",
    "    ax.set_xlabel('Time (s)', fontsize=16)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('Anticipation', fontsize=24)\n",
    "\n",
    "    plt.subplots_adjust(left=0.05, wspace=0.05, hspace=0.3)\n",
    "    plt.savefig('plots/vif/%sreg2_%s.png' %(n_conditions,subject))\n",
    "    plt.close('all')\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Subject Timepoint Censors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from scipy.signal import detrend\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "mri_dir = 'fmri_first_levels'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Timing information.\n",
    "n_acq = 977\n",
    "tr = 1.75\n",
    "\n",
    "## Scrubbing parameters.\n",
    "thresholds = [0.0, 0.5, 0.7, 0.9, 1.1, 1.3]\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define TR onsets.\n",
    "tr_onsets = np.insert( np.cumsum( np.ones(n_acq - 1) * tr ), 0, 0 )\n",
    "\n",
    "## Get subjects list.\n",
    "info = read_csv('demographics.csv')\n",
    "subjects = info.loc[~info.Exlude, 'Subject'].as_matrix()\n",
    "info = open('fmri_second_levels/nuisance_info.csv','w')\n",
    "info.write('Subject,n_mc,FD=0.0,FD=0.5,FD=0.7,FD=0.9,FD=1.1,FD=1.3\\n')\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    info.write('%s,' %subject)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute framewise displacement.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Read motion data.\n",
    "    mc = os.path.join(mri_dir, subject, 'arc_001', '001', 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:7]\n",
    "\n",
    "    ## Invert angular displacement.\n",
    "    fd = mc.copy()\n",
    "    fd[:,:3] = np.deg2rad(fd[:,:3]) \n",
    "    fd[:,:3] *= 50\n",
    "\n",
    "    ## Compute framewise displacement (See Power 2012, 2014).\n",
    "    fd = np.insert( np.abs( np.diff(fd, axis=0) ).sum(axis=1), 0, 0 )\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute motion regressors.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Remove trends.\n",
    "    mc = detrend(mc, axis=0, type='constant')\n",
    "    mc = detrend(mc, axis=0, type='linear')\n",
    "    \n",
    "    ## Perform PCA.\n",
    "    pca = PCA(n_components=6)\n",
    "    mc = pca.fit_transform(mc)\n",
    "    \n",
    "    ## Take only the number of components explaining 90% of the variance.\n",
    "    varexp = np.cumsum(pca.explained_variance_ratio_)\n",
    "    n_components = np.argmax(varexp >= 0.9) + 1\n",
    "    mc = mc[:,:n_components]\n",
    "    \n",
    "    ## Save motion regressor.\n",
    "    f = '%s/%s/arc_001/001/FINAL2.mc.par' %(mri_dir,subject)\n",
    "    np.savetxt(f, mc, fmt='%s')\n",
    "    info.write('%s,' %n_components)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Write scrubbers.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        \n",
    "        ## Find threshold violations.\n",
    "        if not threshold: ix, = np.where(fd >= np.inf)\n",
    "        else: ix, = np.where(fd >= threshold)\n",
    "                \n",
    "        ## Save.\n",
    "        info.write('%s,' %len(ix))\n",
    "        f = '%s/%s/arc_001/001/FINAL2.censor.%s.par' %(mri_dir,subject,threshold)\n",
    "        if len(ix): np.savetxt(f, tr_onsets[ix,np.newaxis], fmt='%s')\n",
    "\n",
    "    info.write('\\n')\n",
    "\n",
    "info.close()\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Censor Analysis: Precompute F maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pandas import read_csv\n",
    "mri_dir = 'fmri_first_levels/concat-sess/FINAL2'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "sm = 6\n",
    "thresholds = [0.0,0.5,0.7,0.9,1.1,1.3]\n",
    "spaces = ['lh','rh','mni305']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup for WLS.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load subject information.\n",
    "info = read_csv('demographics.csv')\n",
    "info = info[~info.Exlude].reset_index()\n",
    "n_subj, _ = info.shape\n",
    "\n",
    "## Build Design Matrix.\n",
    "X = np.zeros((n_subj,2))\n",
    "X[:,0] = 1                                        # Intercept\n",
    "X[:,1] = np.where(info.Scanner == 'Trio', 0, 1)   # Scanner\n",
    "n_subj, n_pred = X.shape\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "def wls(X,Y,W):\n",
    "    B = np.linalg.inv(X.T.dot(W).dot(X)).dot(X.T).dot(W).dot(Y)\n",
    "    ssr = W.dot( np.power(Y - np.dot(X,B),2) ).sum()\n",
    "    scale = ssr / (n_subj - n_pred)\n",
    "    cov_p = np.linalg.inv(X.T.dot(W).dot(X)) * scale\n",
    "    F = np.power(B[0],2) * np.power(cov_p[0,0],-1)\n",
    "    return B[0], F\n",
    "\n",
    "for space in spaces:\n",
    "    \n",
    "    for fd in thresholds:\n",
    "\n",
    "        print space, fd\n",
    "        \n",
    "        results_dir = os.path.join(mri_dir, 'FINAL2.%s.%s.%s' %(sm,fd,space))\n",
    "        \n",
    "        ## Load data.\n",
    "        ces = nib.load(os.path.join(results_dir, 'FINAL2.Delib.par', 'ces.nii.gz')).get_data().squeeze()\n",
    "        cesvar = nib.load(os.path.join(results_dir, 'FINAL2.Delib.par', 'cesvar.nii.gz')).get_data().squeeze()\n",
    "        affine = nib.load(os.path.join(results_dir, 'FINAL2.Delib.par', 'ces.nii.gz')).affine\n",
    "        \n",
    "        ## Reshaping of MNI305 data.\n",
    "        if space == 'mni305':\n",
    "            x,y,z,n_subj = ces.shape\n",
    "            ces = ces.reshape(x*y*z,n_subj)\n",
    "            cesvar = cesvar.reshape(x*y*z,n_subj)\n",
    "        \n",
    "        ## Preallocate arrays for results.\n",
    "        cesvar = np.abs(1./cesvar)\n",
    "        include, = np.where(~np.isinf(cesvar).sum(axis=1).astype(bool))\n",
    "        Fmap = np.repeat(np.nan, ces.shape[0])\n",
    "        \n",
    "        ## Perform WLS.\n",
    "        for i in include:\n",
    "            \n",
    "            ## Update variables\n",
    "            Y = ces[i]\n",
    "            W = np.diag(cesvar[i])\n",
    "            _, Fmap[i] = wls(X,Y,W)\n",
    "        \n",
    "        ## Reshape.\n",
    "        if space == 'mni305': Fmap = Fmap.reshape(x,y,z)\n",
    "        \n",
    "        ## Save.\n",
    "        for _ in range(4 - len(Fmap.shape)): Fmap = np.expand_dims(Fmap, -1)\n",
    "        obj = nib.Nifti1Image(Fmap, affine)\n",
    "        nib.save(obj, os.path.join(results_dir, 'FINAL2.Delib.par', 'F.nii.gz'))\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Censor Analysis: Determine Optimal Threshold\n",
    "Based on the methods from [Siegal et al. (2014)](https://www.ncbi.nlm.nih.gov/pubmed/23861343): *Statistical Improvements in Functional Magnetic Resonance Imaging Analyses Produced by Censoring High-Motion Data Points*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "from mne import read_surface, grow_labels, spatial_tris_connectivity, set_log_level\n",
    "from mne.stats.cluster_level import _find_clusters as find_clusters\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.stats import f as fdist\n",
    "from scipy.ndimage import measurements\n",
    "set_log_level(verbose=False)\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=1.5)\n",
    "%matplotlib inline\n",
    "\n",
    "fs_dir = 'recons'\n",
    "mri_dir = 'fmri_first_levels/concat-sess/FINAL2'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O paramters.\n",
    "sm = 6\n",
    "contrast = 'Delib'\n",
    "censor = True    # {True = Include blobs from all overlays, \n",
    "                 # False = Include blobs from only no-center}\n",
    "\n",
    "## Cluster parameters.\n",
    "cluster_dict = dict(lh = [0.01, 100], rh = [0.01, 100],\n",
    "                    mni305 = [0.01, 20])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define iterators.\n",
    "spaces = ['lh','rh','mni305']\n",
    "thresholds = [0.0, 0.5, 0.7, 0.9, 1.1, 1.3]\n",
    "\n",
    "info = []\n",
    "for n, space in enumerate(spaces):\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    overlays = []\n",
    "    for fd in thresholds: \n",
    "        obj = nib.load(os.path.join(mri_dir, 'FINAL2.%s.%s.%s' %(sm,fd,space), 'FINAL2.%s.par' %contrast, 'F.nii.gz'))\n",
    "        overlays.append( obj.get_data().squeeze() )\n",
    "    overlays = np.array(overlays)\n",
    "\n",
    "    ## Make average map.\n",
    "    if censor: average = overlays.mean(axis=0)\n",
    "    else: average = overlays[0].copy()\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Identify clusters.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    p_value, min_cluster = cluster_dict[space]\n",
    "    min_value = fdist.isf(p_value, 1, 26)\n",
    "    \n",
    "    if space == 'mni305':\n",
    "\n",
    "        masked_average = average > min_value\n",
    "        clusters, n_clusters = measurements.label( masked_average )\n",
    "        clusters = [np.where(clusters==n) for n in np.arange(n_clusters)+1 if (clusters==n).sum() > min_cluster]\n",
    "        \n",
    "    else:\n",
    "\n",
    "        ## Prepare surface information.\n",
    "        _, tris = read_surface(os.path.join(fs_dir, 'fsaverage', 'surf', '%s.white' %space))\n",
    "        connectivity = spatial_tris_connectivity(tris)\n",
    "        include = np.invert(np.isnan(average).astype(bool))\n",
    "        \n",
    "        ## Identify clusters (clusters already sorted by size).\n",
    "        clusters, _ = find_clusters(average, min_value, tail=1, connectivity=connectivity, include=include)\n",
    "        clusters = [c for c in clusters if len(c) > min_cluster]\n",
    "        \n",
    "    print '%s clusters identified for %s.' %(len(clusters), space)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Average across labels / spheres.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    for i, fd in enumerate(thresholds):\n",
    "                    \n",
    "        for j, c in enumerate(clusters):\n",
    "            \n",
    "            fscore = np.nanmean(overlays[i][c])\n",
    "            info.append([fd,space,j,fscore])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Compute statistics.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "info = DataFrame(np.array(info), columns=('FD','Space','Label','Fscore'))\n",
    "info['Fscore'] = info.Fscore.astype(float)\n",
    "print f_oneway(*[info.loc[info.FD==fd,'Fscore'].as_matrix() for fd in np.unique(info.FD)])\n",
    "print info.groupby(['FD',]).Fscore.mean()\n",
    "\n",
    "## Plot.\n",
    "g = sns.factorplot('Space', 'Fscore', 'FD', info, kind='bar', ci=None, size=4, aspect=2);\n",
    "g.ax.set_ylim(12,16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: fMRI Analysis (Second-Levels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precompute Permutations\n",
    "Based on intial calculations, we assume one full loop of WLS + TFCE will take ~17s. We will submit jobs of 100 iterations (approx. 30 minutes time on cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(47404)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "n_subj = 28\n",
    "n_permutations = 5000\n",
    "inc = 100\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Generate permutations.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "permutations = []\n",
    "while True:\n",
    "    arr = np.random.choice([1,-1],n_subj,replace=True)\n",
    "    if not np.any(np.apply_along_axis(np.array_equal, 0, permutations, arr)): \n",
    "        permutations.append(arr)\n",
    "    if len(permutations) >= n_permutations: \n",
    "        break \n",
    "        \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Save.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "permutations = np.array(permutations)\n",
    "index = np.arange(0,n_permutations+1,inc)\n",
    "for n, ix in enumerate(index[1:]): \n",
    "    np.save(os.path.join('fmri_second_levels', 'permutations', 'sign_flips_%s' %(n+1)), permutations[ix-inc:ix])\n",
    "            \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Surface Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from mne import read_label, read_surface, spatial_tris_connectivity, set_log_level\n",
    "set_log_level(verbose=False) \n",
    "subj_dir = '/space/lilli/1/users/DARPA-Recons/fscopy'\n",
    "mri_dir = '/autofs/space/lilli_002/users/DARPA-ARC/NN_bayes_2016/FINAL6/'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "label_dir = os.path.join(subj_dir, 'label', 'dkt40')\n",
    "\n",
    "roi_list = ['caudalanteriorcingulate', 'rostralanteriorcingulate', 'posteriorcingulate',\n",
    "            'superiorfrontal', 'medialorbitofrontal', 'rostralmiddlefrontal', 'caudalmiddlefrontal',\n",
    "            'parsopercularis', 'parstriangularis', 'parsorbitalis', 'lateralorbitofrontal', 'insula']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Make labels.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "\n",
    "for hemi in ['lh', 'rh']:\n",
    "    \n",
    "    ## Assemble and merge labels.\n",
    "    label = []\n",
    "    for roi in roi_list: label.append(read_label(os.path.join(label_dir,'%s-%s.label' %(roi,hemi))))\n",
    "    label = np.sum(label)\n",
    "    \n",
    "    ## Save label.\n",
    "    label.name = 'arc-%s' %hemi\n",
    "    label.save('fmri_second_levels/arc-%s.label' %hemi)\n",
    "    \n",
    "    ## Load surface.\n",
    "    _, tris = read_surface(os.path.join(subj_dir, 'surf', '%s.white' %hemi))\n",
    "    mapping = np.in1d(np.unique(tris),label.vertices)\n",
    "    \n",
    "    ## Reduce triangles to those in label.\n",
    "    ix = np.all(np.apply_along_axis(np.in1d, 0, tris, label.vertices), axis=1)\n",
    "    tris = tris[ix]\n",
    "\n",
    "    ## Compute connectivity.\n",
    "    coo = spatial_tris_connectivity(tris, remap_vertices=True)\n",
    "    np.savez('fmri_second_levels/%s_connectivity' %hemi, data = coo.data, row = coo.row,\n",
    "             col = coo.col, shape = coo.shape, mapping=mapping, vertices=label.vertices)\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Volume Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from scipy.sparse import coo_matrix\n",
    "lut = '/usr/local/freesurfer/stable5_3_0/FreeSurferColorLUT.txt'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define subcortical structures of interest.\n",
    "roi_dict = {18:'Left-Amygdala', 11:'Left-Caudate', 17:'Left-Hippocampus', 12:'Left-Putamen', \n",
    "            54:'Right-Amygdala', 50:'Right-Caudate', 53:'Right-Hippocampus', 51:'Right-Putamen'}\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Create mask.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load aseg.\n",
    "aseg = '/space/lilli/1/users/DARPA-Recons/fsaverage/mri.2mm/aseg.mgz'\n",
    "aseg = nib.load(aseg).get_data()\n",
    "\n",
    "## Find all voxels in ROI list. Get corresponding labels.\n",
    "mapping = np.in1d(aseg, roi_dict.keys()).reshape(aseg.shape)\n",
    "voxels = np.where(mapping)\n",
    "names = np.array([roi_dict[i] for i in aseg[voxels]])\n",
    "voxels = np.vstack(voxels).T\n",
    "\n",
    "## Initialize connectivity matrix.\n",
    "n_voxels, _ = voxels.shape\n",
    "coo = np.zeros([n_voxels,n_voxels], dtype=int)\n",
    "\n",
    "## Iteratively test for adjacency.\n",
    "## Here we use 6-lattice connectivity (up,down,forward,backward,left,right).\n",
    "for n in range(n_voxels):\n",
    "    diff = np.linalg.norm(voxels - voxels[n], axis=1)\n",
    "    M, = np.where(diff==1.)\n",
    "    for m in M: coo[n,m] = 1 \n",
    "coo = coo_matrix(coo)\n",
    "    \n",
    "## Save.\n",
    "np.savez('fmri_second_levels/mni305_connectivity', data = coo.data, row = coo.row,\n",
    "         col = coo.col, shape = coo.shape, mapping=mapping, voxels=voxels, names=names)\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Mean Signal from ROIs\n",
    "Necessary for computing percent signal change down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pandas import read_csv\n",
    "mri_dir = 'fmri_first_levels/'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "\n",
    "n_acq = 977\n",
    "tr = 1.75\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Get subjects list.\n",
    "info = read_csv('demographics.csv')\n",
    "subjects = info.loc[~info.Exlude, 'Subject'].as_matrix()\n",
    "\n",
    "## Define TR onsets.\n",
    "tr_onsets = np.insert( np.cumsum( np.ones(n_acq - 1) * tr ), 0, 0 )\n",
    "\n",
    "mean_signal = dict()\n",
    "for space in ['lh','rh','mni305']:\n",
    "    \n",
    "    print space,\n",
    "    \n",
    "    ## Load masks.\n",
    "    npz = np.load('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "    include = npz['mapping']\n",
    "    \n",
    "    ## Preallocate space.\n",
    "    ms = np.zeros([len(subjects), include.sum()])\n",
    "    \n",
    "    ## Iterate over subjects.\n",
    "    for n, subject in enumerate(subjects):\n",
    "        \n",
    "        ## Load data.\n",
    "        subj_dir = os.path.join(mri_dir, subject, 'arc_001', '001')\n",
    "        if space == 'mni305': f = os.path.join(subj_dir,'fmcpr.sm%s.%s.2mm.b0dc.nii.gz' %(sm,space))\n",
    "        else: f = os.path.join(subj_dir,'fmcpr.sm%s.fsaverage.%s.b0dc.nii.gz' %(sm,space))\n",
    "        data = nib.load(f).get_data()\n",
    "        \n",
    "        ## Censor data. Average across acquisitions.\n",
    "        try: censor = np.loadtxt(os.path.join(subj_dir, 'FINAL.censor.%s.par' %fd))\n",
    "        except IOError: censor = []\n",
    "        censor = np.invert(np.in1d(tr_onsets, censor))\n",
    "        \n",
    "        data = data[include,...].squeeze()\n",
    "        data = data[...,censor].mean(axis=1)\n",
    "        \n",
    "        ## Append.\n",
    "        ms[n] = data\n",
    "    \n",
    "    ## Store in dictionary.\n",
    "    mean_signal[space] = ms\n",
    "    \n",
    "## Save.\n",
    "f = 'fmri_second_levels/mean_signal'\n",
    "np.savez_compressed(f, lh = mean_signal['lh'], rh = mean_signal['rh'], mni305 = mean_signal['mni305'])\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from mne import read_label\n",
    "from pandas import read_csv\n",
    "mri_dir = 'fmri_first_levels/concat-sess/FINAL2/'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "sm = 6\n",
    "spaces = ['lh','rh','mni305']\n",
    "thresholds = [0.9]\n",
    "contrasts = ['Delib', 'DelibMod', 'Antcp', 'AntcpMod', 'Shock']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for space in spaces:\n",
    "\n",
    "    ## Load masks.\n",
    "    npz = np.load('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "    include = npz['mapping']\n",
    "    \n",
    "    for fd in thresholds:\n",
    "                \n",
    "        results_dir = os.path.join(mri_dir, 'FINAL2.%s.%s.%s' %(sm,fd,space))\n",
    "            \n",
    "        for contrast in contrasts:\n",
    "        \n",
    "            ## Make save directory.\n",
    "            out_dir = os.path.join('fmri_second_levels', 'FINAL2.%s.%s.%s.%s' %(sm,fd,space,contrast))\n",
    "            if not os.path.isdir(out_dir): os.makedirs(out_dir)\n",
    "    \n",
    "            ## Load data.\n",
    "            ces = nib.load(os.path.join(results_dir, 'FINAL2.%s.par' %contrast, 'ces.nii.gz')).get_data()\n",
    "            cesvar = nib.load(os.path.join(results_dir, 'FINAL2.%s.par' %contrast, 'cesvar.nii.gz')).get_data()\n",
    "            affine = nib.load(os.path.join(results_dir, 'FINAL2.%s.par' %contrast, 'ces.nii.gz')).affine\n",
    "            \n",
    "            ## Masking.\n",
    "            ces = ces[include,...]\n",
    "            cesvar = cesvar[include,...]\n",
    "                    \n",
    "            ## Save.\n",
    "            np.savez_compressed(os.path.join(out_dir, 'first_levels'), ces=ces.squeeze(), cesvar=cesvar.squeeze())\n",
    "            np.save(os.path.join(out_dir, 'affine'), affine)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform WLS Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from scipy.sparse import coo_matrix\n",
    "from mne.stats.cluster_level import _find_clusters as find_clusters\n",
    "root_dir = '/space/sophia/2/users/DARPA-Behavior/notebooks/bayes/decision_making/NN_bayes_2016'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "space = 'mni305'\n",
    "contrast = 'Delib'\n",
    "\n",
    "## Permutation parameters.\n",
    "permutations = 0\n",
    "\n",
    "## TFCE parameters.\n",
    "threshold = dict(start=0.1, step=0.1, h_power=2, e_power=0.5)\n",
    "tail = 0\n",
    "max_step = 1\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "def load_sparse_coo(filename):\n",
    "    npz = np.load(filename)\n",
    "    M,N = npz['shape']\n",
    "    return coo_matrix( (npz['data'], (npz['row'],npz['col'])), (M,N) )\n",
    "\n",
    "out_dir = os.path.join(root_dir, 'fmri_second_levels', 'FINAL.%s.%s.%s.%s' %(sm,fd,space,contrast))\n",
    "\n",
    "## Load data.\n",
    "npz = np.load(os.path.join(out_dir, 'first_levels.npz'))\n",
    "ces = npz['ces']\n",
    "cesvar = np.abs( 1. / npz['cesvar'] )\n",
    "\n",
    "## Define indices.\n",
    "connectivity = load_sparse_coo(os.path.join(root_dir, 'fmri_second_levels', '%s_connectivity.npz' %space))\n",
    "index,  = np.where(~np.isinf(cesvar).sum(axis=1).astype(bool))\n",
    "include = ~np.isinf(cesvar).sum(axis=1).astype(bool)\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup for permutation testing.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load subject information.\n",
    "info = read_csv(os.path.join(root_dir, 'demographics.csv'))\n",
    "info = info[~info.Exlude].reset_index()\n",
    "n_subj, _ = info.shape\n",
    "\n",
    "## Build Design Matrix.\n",
    "X = np.zeros((n_subj,2))\n",
    "X[:,0] = 1                                        # Intercept\n",
    "X[:,1] = np.where(info.Scanner == 'Trio', 0, 1)   # Scanner\n",
    "n_subj, n_pred = X.shape\n",
    "\n",
    "## If specified, load precomputed sign flips.\n",
    "if permutations: sign_flips = np.load(os.path.join(root_dir, 'fmri_second_levels', 'permutations', 'sign_flips_%s.npy' %permutations))\n",
    "else: sign_flips = np.ones((1,n_subj))\n",
    "n_shuffles = sign_flips.shape[0]\n",
    "\n",
    "## Preallocate arrays for results.\n",
    "shape = [n_shuffles] + list(ces.shape[:-1])\n",
    "Bmap = np.zeros(shape)\n",
    "Fmap = np.zeros(shape)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "'''\n",
    "Following the instructions of Winkler et al. (2014), we use the Freedman and Lane (1983)\n",
    "permutation procedure. This allows us to precompute a number of values ahead of time.\n",
    "\n",
    "To understand the WLS computations, please see:\n",
    "https://github.com/statsmodels/statsmodels/blob/master/statsmodels/base/model.py\n",
    "https://github.com/statsmodels/statsmodels/blob/master/statsmodels/regression/linear_model.py\n",
    "'''\n",
    "\n",
    "def wls(X,Y,W):\n",
    "    B = np.linalg.inv(X.T.dot(W).dot(X)).dot(X.T).dot(W).dot(Y)\n",
    "    ssr = W.dot( np.power(Y - np.dot(X,B),2) ).sum()\n",
    "    scale = ssr / (n_subj - n_pred)\n",
    "    cov_p = np.linalg.inv(X.T.dot(W).dot(X)) * scale\n",
    "    F = np.power(B[0],2) * np.power(cov_p[0,0],-1)\n",
    "    return B[0], F\n",
    "\n",
    "## Loop it!\n",
    "for n, sf in enumerate(sign_flips):\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute statistics.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    for m in index:\n",
    "\n",
    "        ## Update variables.\n",
    "        W = np.diag(cesvar[m])\n",
    "        Y = ces[m]\n",
    "        \n",
    "        ## Permute values.\n",
    "        ## See Winkler et al. (2014), pg. 385\n",
    "        ## To compute Hat Matrix, see: https://en.wikipedia.org/wiki/Projection_matrix and \n",
    "        Z = X[:,1:]\n",
    "        ZZ = Z.dot( np.linalg.inv( Z.T.dot(W).dot(Z) ) ).dot(Z.T).dot(W)\n",
    "        Rz = np.identity(n_subj) - ZZ\n",
    "        Y = np.diag(sf).dot(Rz).dot(Y)\n",
    "        \n",
    "        ## Perform WLS.\n",
    "        Bmap[n,m], Fmap[n,m] = wls(X,Y,W) \n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Perform TFCE.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    _, Fmap[n] = find_clusters(Fmap[n], threshold, tail=tail, connectivity=connectivity, \n",
    "                               include=include, max_step=max_step, show_info=False)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Save results.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#  \n",
    "\n",
    "if not permutations: f = os.path.join(out_dir, '%s_%s_obs' %(space, contrast))\n",
    "else: f = os.path.join(out_dir, '%s_%s_perm-%s' %(space, contrast, permutations)) \n",
    "np.savez_compressed(f, Bmap=Bmap, Fmap=Fmap)\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform FWE Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "def prepare_image(arr, space):\n",
    "    npz = np.load('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "    image = np.zeros_like(npz['mapping'], dtype=float)\n",
    "    \n",
    "    if not space == 'mni305': \n",
    "        image[npz['vertices']] += arr\n",
    "    else:\n",
    "        x,y,z = npz['voxels'].T\n",
    "        image[x,y,z] += arr\n",
    "    \n",
    "    for _ in range(4 - len(image.shape)): image = np.expand_dims(image,-1)\n",
    "    return image\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "spaces = ['lh','rh','mni305']\n",
    "contrasts = ['Delib','DelibMod','Antcp','AntcpMod','Shock']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "permutations = np.arange(50) + 1\n",
    "\n",
    "for contrast in contrasts:\n",
    "\n",
    "    print contrast,\n",
    "    \n",
    "    for n, space in enumerate(spaces):\n",
    "    \n",
    "        out_dir = 'fmri_second_levels/FINAL2.%s.%s.%s.%s' %(sm, fd, space, contrast)\n",
    "        \n",
    "        ## Load true effects.\n",
    "        npz = np.load(os.path.join(out_dir, '%s_%s_obs.npz' %(space,contrast)))\n",
    "        Bmap = npz['Bmap'].squeeze()\n",
    "        Fmap = npz['Fmap'].squeeze()\n",
    "        \n",
    "        ## Load permutations.\n",
    "        Pmap = []\n",
    "        for p in permutations: \n",
    "            npz = np.load(os.path.join(out_dir, '%s_%s_perm-%s.npz' %(space,contrast,p)))\n",
    "            Pmap.append(npz['Fmap'])\n",
    "        Pmap = np.concatenate(Pmap, axis=0)\n",
    "        n_permutations, _ = Pmap.shape\n",
    "\n",
    "        ## Compute p-values via FWE.\n",
    "        p_values = np.ones_like(Fmap)\n",
    "        for mp in Pmap.max(axis=1): p_values += mp > Fmap\n",
    "        p_values /= n_permutations + 1.\n",
    "        p_values = -np.log10(p_values) * np.sign(Bmap)\n",
    "      \n",
    "        ## Save maps.\n",
    "        np.save(os.path.join(out_dir,'%s_%s_fwe' %(space,contrast)), p_values)\n",
    "        for arr, name in zip([Bmap,Fmap,p_values],['beta','F','fwe']):\n",
    "            image = prepare_image(arr, space)\n",
    "            image = nib.Nifti1Image(image, np.load(os.path.join(out_dir,'affine.npy')))\n",
    "            nib.save(image, os.path.join(out_dir, '%s.nii.gz' %name))\n",
    "            \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform FDR Corrections\n",
    "Not used, but programmed for example's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from mne.stats import fdr_correction\n",
    "\n",
    "def prepare_image(arr, space):\n",
    "    npz = np.load('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "    image = np.zeros_like(npz['mapping'], dtype=float)\n",
    "    \n",
    "    if not space == 'mni305': \n",
    "        image[npz['vertices']] += arr\n",
    "    else:\n",
    "        x,y,z = npz['voxels'].T\n",
    "        image[x,y,z] += arr\n",
    "    \n",
    "    for _ in range(4 - len(image.shape)): image = np.expand_dims(image,-1)\n",
    "    return image\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "spaces = ['lh','rh','mni305']\n",
    "contrasts = ['Delib','DelibMod','Antcp','AntcpMod','Shock']\n",
    "contrasts = ['DelibMod']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "permutations = np.arange(50) + 1\n",
    "\n",
    "for contrast in contrasts:\n",
    "\n",
    "    print contrast,\n",
    "    FDR, signs = [], []\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute p-values within spaces.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    for n, space in enumerate(spaces):\n",
    "    \n",
    "        out_dir = 'fmri_second_levels/FINAL.%s.%s.%s.%s' %(sm, fd, space, contrast)\n",
    "        \n",
    "        ## Load true effects.\n",
    "        npz = np.load(os.path.join(out_dir, '%s_%s_obs.npz' %(space,contrast)))\n",
    "        Bmap = npz['Bmap'].squeeze()\n",
    "        Fmap = npz['Fmap'].squeeze()\n",
    "        \n",
    "        ## Load permutations.\n",
    "        Pmap = []\n",
    "        for p in permutations: \n",
    "            npz = np.load(os.path.join(out_dir, '%s_%s_perm-%s.npz' %(space,contrast,p)))\n",
    "            Pmap.append(npz['Fmap'])\n",
    "        Pmap = np.concatenate(Pmap, axis=0)\n",
    "        n_permutations, _ = Pmap.shape\n",
    "\n",
    "        ## Compute p-values via FWE.\n",
    "        p_values = (Pmap >= Fmap).sum(axis=0) + 1.\n",
    "        p_values /= n_permutations + 1.\n",
    "        FDR.append(p_values)\n",
    "        signs.append(np.sign(Bmap))\n",
    "    \n",
    "        ## Save maps.\n",
    "        for arr, name in zip([Bmap,Fmap],['beta','F']):\n",
    "            image = prepare_image(arr, space)\n",
    "            image = nib.Nifti1Image(image, np.load(os.path.join(out_dir,'affine.npy')))\n",
    "            nib.save(image, os.path.join(out_dir, '%s.nii.gz' %name))        \n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Perform FDR corrections.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "            \n",
    "    ## Assemble info.\n",
    "    indices = np.concatenate([np.ones_like(arr) * n for n, arr in enumerate(FDR)])\n",
    "    FDR = np.concatenate(FDR)\n",
    "    signs = np.concatenate(signs)\n",
    "\n",
    "    ## Perform FDR correction.\n",
    "    FDR[np.where(signs)] = fdr_correction(FDR[np.where(signs)])[-1]\n",
    "    FDR = -np.log10(FDR) * signs\n",
    "\n",
    "    ## Save maps.\n",
    "    for n, space in enumerate(spaces):\n",
    "        out_dir = 'fmri_second_levels/FINAL.%s.%s.%s.%s' %(sm, fd, space, contrast)\n",
    "        np.save(os.path.join(out_dir,'%s_%s_fdr' %(space,contrast)), FDR[indices==n])\n",
    "        image = prepare_image(FDR[indices==n], space)\n",
    "        image = nib.Nifti1Image(image, np.load(os.path.join(out_dir,'affine.npy')))\n",
    "        nib.save(image, os.path.join(out_dir, 'fdr.nii.gz'))\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Second-Level Maps\n",
    "Thresholding clusters such that:\n",
    "* p < 0.05 (FWE corrected, alpha = 0.05)\n",
    "* Surface: clusters > 100mm2\n",
    "* Volume: clusters > 20 contiguous voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pandas import DataFrame\n",
    "from scipy.sparse import coo_matrix\n",
    "from mne.stats.cluster_level import _find_clusters as find_clusters\n",
    "fs_dir = 'recons'\n",
    "\n",
    "def load_sparse_coo(filename):\n",
    "    npz = np.load(filename)\n",
    "    M,N = npz['shape']\n",
    "    return coo_matrix( (npz['data'], (npz['row'],npz['col'])), (M,N) )\n",
    "\n",
    "def prepare_image(arr, space):\n",
    "    npz = np.load('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "    image = np.zeros_like(npz['mapping'], dtype=float)\n",
    "    \n",
    "    if not space == 'mni305': \n",
    "        image[npz['vertices']] += arr\n",
    "    else:\n",
    "        x,y,z = npz['voxels'].T\n",
    "        image[x,y,z] += arr\n",
    "    \n",
    "    for _ in range(4 - len(image.shape)): image = np.expand_dims(image,-1)\n",
    "    return image\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "spaces = ['lh','rh','mni305']\n",
    "contrasts = ['Delib','DelibMod','Antcp','AntcpMod','Shock']\n",
    "\n",
    "## Thresholding parameters.\n",
    "threshold = -np.log10( 0.05 )\n",
    "min_cluster = dict(lh = 100, rh = 100, mni305 = 20)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for space in spaces:\n",
    "    \n",
    "    print space,\n",
    "    \n",
    "    ## Load connectivity information.\n",
    "    connectivity = load_sparse_coo('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "\n",
    "    ## Load mapping information.\n",
    "    npz = np.load('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "    \n",
    "    if not space == 'mni305':\n",
    "        vertices = npz['vertices']\n",
    "        average_area = nib.load(os.path.join(fs_dir, 'fsaverage', 'surf', '%s.white.avg.area.mgh' %space)).get_data()\n",
    "        average_area = average_area[vertices].squeeze()\n",
    "        \n",
    "    for contrast in contrasts:\n",
    "        \n",
    "        ## Load FWE-corrected p-values.\n",
    "        f = 'fmri_second_levels/FINAL2.%s.%s.%s.%s/%s_%s_fwe.npy' %(sm, fd, space, contrast, space, contrast)\n",
    "        fwe = np.load(f)\n",
    "        \n",
    "        ## Find clusters.\n",
    "        include = np.where(fwe,True,False)\n",
    "        clusters, sums = find_clusters(fwe, threshold, tail=0, connectivity=connectivity, include=include, t_power=0)\n",
    "        \n",
    "        ## Compute areas.\n",
    "        if not space == 'mni305': cluster_sums = np.array([average_area[c].sum() for c in clusters])\n",
    "        else: cluster_sums = sums\n",
    "            \n",
    "        ## Threshold.\n",
    "        try:\n",
    "            survival_ix = np.concatenate([c for c, s in zip(clusters,cluster_sums) if s > min_cluster[space]])\n",
    "            fwe[~np.in1d(np.arange(fwe.shape[0]), survival_ix)] = 0\n",
    "        except ValueError:\n",
    "            fwe = np.zeros_like(fwe)\n",
    "        \n",
    "        ## Save.\n",
    "        image = prepare_image(fwe, space)\n",
    "        image = nib.Nifti1Image(image, np.load(os.path.join(os.path.dirname(f),'affine.npy')))\n",
    "        nib.save(image, os.path.join(os.path.dirname(f), 'fwe_thresh_%0.3f.nii.gz' %threshold))\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Percent Signal Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pandas import read_csv\n",
    "mri_dir = 'fmri_first_levels'\n",
    "\n",
    "def prepare_image(arr, space):\n",
    "    npz = np.load('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "    image = np.zeros_like(npz['mapping'], dtype=float)\n",
    "    \n",
    "    if not space == 'mni305': \n",
    "        image[npz['vertices']] += arr\n",
    "    else:\n",
    "        x,y,z = npz['voxels'].T\n",
    "        image[x,y,z] += arr\n",
    "    \n",
    "    for _ in range(4 - len(image.shape)): image = np.expand_dims(image,-1)\n",
    "    return image\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "spaces = ['lh','rh','mni305']\n",
    "contrasts = ['Delib','DelibMod','Antcp','AntcpMod','Shock']\n",
    "threshold = 1.301\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main Loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Get subjects list.\n",
    "info = read_csv('demographics.csv')\n",
    "subjects = info.loc[~info.Exlude, 'Subject'].as_matrix()\n",
    "\n",
    "## Load average signal.\n",
    "mean_signal = np.load('fmri_second_levels/mean_signal.npz')\n",
    "\n",
    "for space in spaces:\n",
    "    \n",
    "    print space,\n",
    "    \n",
    "    ## Assemble design matrices.\n",
    "    subj_dir = os.path.join(mri_dir, '%s', 'arc_001', 'FINAL2.%s.%s.%s' %(sm, fd, space), 'X.dat')\n",
    "    scale_factors = np.array([np.loadtxt(subj_dir %subject).max(axis=0)[:len(contrasts)] \n",
    "                             for subject in subjects]).T\n",
    "\n",
    "    for n, contrast in enumerate(contrasts):\n",
    "        \n",
    "        ## Load first levels.\n",
    "        out_dir = 'fmri_second_levels/FINAL2.%s.%s.%s.%s' %(sm, fd, space, contrast)\n",
    "        ces = np.load(os.path.join(out_dir, 'first_levels.npz'))['ces']\n",
    "        \n",
    "        ## Compute PSC (Pernet 2014, Frontiers in Neuroscience).\n",
    "        ms = np.where(mean_signal[space], mean_signal[space], np.inf).T\n",
    "        psc = np.divide(ces * scale_factors[n] * 100., ms)\n",
    "        psc = prepare_image(psc.mean(axis=1), space)\n",
    "        \n",
    "        ## Mask image.\n",
    "        fwe = nib.load(os.path.join(out_dir, 'fwe_thresh_%s.nii.gz' %threshold)).get_data()\n",
    "        psc *= np.where(fwe,1,0)\n",
    "        \n",
    "        ## Save.\n",
    "        image = nib.Nifti1Image(psc, np.load(os.path.join(out_dir,'affine.npy')))\n",
    "        nib.save(image, os.path.join(out_dir, 'psc.nii.gz'))\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "from surfer import Brain\n",
    "img_dir = 'plots/FINAL2/second_levels'\n",
    "%matplotlib qt4\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "contrasts = ['Delib']\n",
    "overlay = 'psc'\n",
    "surface = 'inflated'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for hemi in ['lh','rh']: \n",
    "\n",
    "    for contrast in contrasts:\n",
    "\n",
    "        fn = os.path.join('fmri_second_levels', 'FINAL.%s.%s.%s.%s' %(sm, fd, hemi, contrast), '%s.nii.gz' %overlay)\n",
    "        \n",
    "        for view in ['lateral','medial']:\n",
    "            \n",
    "            brain = Brain(\"fsaverage\", hemi, surface)\n",
    "            try: brain.add_overlay(fn, min=0.04, max=2.5, sign=\"pos\")\n",
    "            except: continue\n",
    "            brain.show_view(view=view)\n",
    "            od = os.path.join(img_dir, overlay, surface)\n",
    "            if not os.path.isdir(od): os.makedirs(od)\n",
    "            of = os.path.join(od, '%s_%s_%s.png' %(hemi,contrast,view))\n",
    "            Brain.save_image(brain,of)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute surface summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from mne import Label, read_label, grow_labels, vertex_to_mni, set_log_level\n",
    "set_log_level(verbose=False)\n",
    "fs_dir = 'recons'\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "contrast = 'DelibMod'\n",
    "thresh = 1.301\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "\n",
    "## ROI parameters.\n",
    "extent = 10 #mm\n",
    "grow = False\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "label_dir = 'fmri_second_levels/labels/seeds_%s' %contrast\n",
    "labels = sorted([f for f in os.listdir(label_dir) if not f.startswith('fig') and f.endswith('label')])\n",
    "\n",
    "fmni = open('%s/%s_surface_mni2.csv' %(label_dir,contrast), 'w')\n",
    "fmni.write(','.join(['Label','V','X','Y','Z','PSC','F','p'])+'\\n')\n",
    "\n",
    "for f in labels:\n",
    "\n",
    "    ## Extract info. Read label.\n",
    "    roi, hemi = f.replace('.label', '').split('-')\n",
    "    label = read_label(os.path.join(label_dir, f))\n",
    "    \n",
    "    ## Load accompanying overlay.\n",
    "    f = 'fmri_second_levels/FINAL2.%s.%s.%s.%s/psc.nii.gz' %(sm,fd,hemi,contrast)\n",
    "    overlay = nib.load(f).get_data().squeeze()\n",
    "\n",
    "    ## Find maximum vertex.\n",
    "    ix = np.argmax(overlay[label.vertices])\n",
    "    v = label.vertices[ix]\n",
    "\n",
    "    ## Extract MNI coordinates.\n",
    "    x,y,z = vertex_to_mni(v, 0 if hemi=='lh' else 1, 'fsaverage', fs_dir)[0]\n",
    "    \n",
    "    ## Extract PSC, F-scores, p-values.\n",
    "    f = 'fmri_second_levels/FINAL2.%s.%s.%s.%s/psc.nii.gz' %(sm,fd,hemi,contrast)\n",
    "    psc = nib.load(f).get_data().squeeze()[v]\n",
    "\n",
    "    f = 'fmri_second_levels/FINAL2.%s.%s.%s.%s/F.nii.gz' %(sm,fd,hemi,contrast)\n",
    "    F = nib.load(f).get_data().squeeze()[v]\n",
    "    \n",
    "    f = 'fmri_second_levels/FINAL2.%s.%s.%s.%s/fwe_thresh_%s.nii.gz' %(sm,fd,hemi,contrast,thresh)\n",
    "    p = nib.load(f).get_data().squeeze()[v]\n",
    "    \n",
    "    ## Write information.\n",
    "    fmni.write('%s-%s,%s,%0.0f,%0.0f,%0.0f,%0.2f,%0.2f,%0.6f\\n' %(roi,hemi,v,x,y,z,psc,F,10.**-p))\n",
    "    \n",
    "    if grow:\n",
    "        \n",
    "        ## Grow label.\n",
    "        label = grow_labels('fsaverage', v, extent, 0 if hemi=='lh' else 1, subjects_dir=fs_dir,\n",
    "                            names='fig_%s-%s' %(roi,hemi), surface='pial')[0]\n",
    "        \n",
    "        ## Ensure label is within actiation. Save.\n",
    "        ix = np.in1d(label.vertices, np.where(overlay)[0])\n",
    "        label.pos = label.pos[ix]\n",
    "        label.values = label.values[ix]\n",
    "        label.vertices = label.vertices[ix]\n",
    "        label.save('%s/%s.label' %(label_dir, label.name))\n",
    "    \n",
    "fmni.close()\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute volume summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nibabel.affines import apply_affine\n",
    "fs_dir = '/space/lilli/1/users/DARPA-Recons'\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "contrast = 'Delib'\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "\n",
    "## ROI parameters.\n",
    "extent = 6 #mm\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "label_dir = 'fmri_second_levels/labels/seeds_%s' %contrast\n",
    "\n",
    "## Initialize statistics file.\n",
    "fmni = open('%s/%s_volume_mni2.csv' %(label_dir,contrast), 'w')\n",
    "fmni.write(','.join(['Label','cX','cY','cZ','X','Y','Z','PSC','F','p'])+'\\n')\n",
    "\n",
    "## Load data. \n",
    "npz = np.load('fmri_second_levels/mni305_connectivity.npz')\n",
    "affine = np.load('fmri_second_levels/FINAL2.%s.%s.mni305.%s/affine.npy' %(sm,fd,contrast))\n",
    "obj = nib.load('fmri_second_levels/FINAL2.%s.%s.mni305.%s/psc.nii.gz' %(sm,fd,contrast))\n",
    "\n",
    "overlay = obj.get_data().squeeze()\n",
    "Fval = nib.load('fmri_second_levels/FINAL2.%s.%s.mni305.%s/F.nii.gz' %(sm,fd,contrast)).get_data().squeeze()\n",
    "pval = nib.load('fmri_second_levels/FINAL2.%s.%s.mni305.%s/fwe_thresh_1.301.nii.gz' %(sm,fd,contrast)).get_data().squeeze()\n",
    "\n",
    "rois = ['Left-Caudate', 'Left-Putamen', 'Left-Hippocampus',\n",
    "        'Right-Caudate', 'Right-Putamen', 'Right-Hippocampus']\n",
    "for roi in rois:\n",
    "    \n",
    "    ## Extract activated voxels in ROI.\n",
    "    voxels = npz['voxels'][npz['names'] == roi]\n",
    "    voxels = voxels[np.where(overlay[[arr for arr in voxels.T]])]\n",
    "    \n",
    "    ## Find maximally activated voxel.\n",
    "    ix = np.argmax(overlay[[arr for arr in voxels.T]])\n",
    "    center = voxels[ix]\n",
    "    i,j,k = center\n",
    "    \n",
    "    ## Get MNI coordinates.\n",
    "    x,y,z = apply_affine(affine, center)\n",
    "    \n",
    "    ## Extract max values.\n",
    "    psc = overlay[i,j,k]\n",
    "    F = Fval[i,j,k]\n",
    "    p = pval[i,j,k]\n",
    "    \n",
    "    ## Write to file.\n",
    "    fmni.write('%s,%0.0d,%0.0d,%0.0d,%0.0d,%0.2d,%0.2d,%0.2f,%0.2f,%0.6f\\n' %(roi,i,j,k,x,y,z,psc,F,10.**-p))\n",
    "    \n",
    "    ## Create sphere: find all voxels within extent.\n",
    "    dist = [np.linalg.norm( np.diff( apply_affine(affine,np.vstack([center,v])), axis=0 ) ) for v in voxels]\n",
    "    ix = np.where(np.array(dist)<=extent)\n",
    "    sphere = voxels[ix]\n",
    "    \n",
    "    ## Save.\n",
    "    #hemi, roi = roi.split('-')\n",
    "    #if hemi.startswith('L'): name = '%s-lh' %roi.lower()\n",
    "    #else: name = '%s-rh' %roi.lower()\n",
    "    #np.save(os.path.join(out_dir, name), sphere)\n",
    "    \n",
    "fmni.close()\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-hoc F-statistic Fix\n",
    "Sam realized very late in the game he should have been saving out the pre-TFCE F-statistics. Fortunately these can be recomputed using the WLS code sans TFCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szoro/Documents/software/anaconda2.7/lib/python2.7/site-packages/ipykernel/__main__.py:30: RuntimeWarning: divide by zero encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "space = 'lh'\n",
    "contrast = 'DelibMod'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "def load_sparse_coo(filename):\n",
    "    npz = np.load(filename)\n",
    "    M,N = npz['shape']\n",
    "    return coo_matrix( (npz['data'], (npz['row'],npz['col'])), (M,N) )\n",
    "\n",
    "out_dir = os.path.join('fmri_second_levels', 'FINAL.%s.%s.%s.%s' %(sm,fd,space,contrast))\n",
    "\n",
    "## Load data.\n",
    "npz = np.load(os.path.join(out_dir, 'first_levels.npz'))\n",
    "ces = npz['ces']\n",
    "cesvar = np.abs( 1. / npz['cesvar'] )\n",
    "\n",
    "## Define indices.\n",
    "connectivity = load_sparse_coo(os.path.join('fmri_second_levels', '%s_connectivity.npz' %space))\n",
    "index,  = np.where(~np.isinf(cesvar).sum(axis=1).astype(bool))\n",
    "include = ~np.isinf(cesvar).sum(axis=1).astype(bool)\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Setup for permutation testing.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load subject information.\n",
    "info = read_csv(os.path.join('demographics.csv'))\n",
    "info = info[~info.Exlude].reset_index()\n",
    "n_subj, _ = info.shape\n",
    "\n",
    "## Build Design Matrix.\n",
    "X = np.zeros((n_subj,2))\n",
    "X[:,0] = 1                                        # Intercept\n",
    "X[:,1] = np.where(info.Scanner == 'Trio', 0, 1)   # Scanner\n",
    "n_subj, n_pred = X.shape\n",
    "\n",
    "sign_flips = np.ones((1,n_subj))\n",
    "n_shuffles = sign_flips.shape[0]\n",
    "\n",
    "## Preallocate arrays for results.\n",
    "shape = [n_shuffles] + list(ces.shape[:-1])\n",
    "Bmap = np.zeros(shape)\n",
    "Fmap = np.zeros(shape)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "'''\n",
    "Following the instructions of Winkler et al. (2014), we use the Freedman and Lane (1983)\n",
    "permutation procedure. This allows us to precompute a number of values ahead of time.\n",
    "\n",
    "To understand the WLS computations, please see:\n",
    "https://github.com/statsmodels/statsmodels/blob/master/statsmodels/base/model.py\n",
    "https://github.com/statsmodels/statsmodels/blob/master/statsmodels/regression/linear_model.py\n",
    "'''\n",
    "\n",
    "def wls(X,Y,W):\n",
    "    B = np.linalg.inv(X.T.dot(W).dot(X)).dot(X.T).dot(W).dot(Y)\n",
    "    ssr = W.dot( np.power(Y - np.dot(X,B),2) ).sum()\n",
    "    scale = ssr / (n_subj - n_pred)\n",
    "    cov_p = np.linalg.inv(X.T.dot(W).dot(X)) * scale\n",
    "    F = np.power(B[0],2) * np.power(cov_p[0,0],-1)\n",
    "    return B[0], F\n",
    "\n",
    "## Loop it!\n",
    "for n, sf in enumerate(sign_flips):\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute statistics.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    for m in index:\n",
    "\n",
    "        ## Update variables.\n",
    "        W = np.diag(cesvar[m])\n",
    "        Y = ces[m]\n",
    "        \n",
    "        ## Permute values.\n",
    "        ## See Winkler et al. (2014), pg. 385\n",
    "        ## To compute Hat Matrix, see: https://en.wikipedia.org/wiki/Projection_matrix and \n",
    "        Z = X[:,1:]\n",
    "        ZZ = Z.dot( np.linalg.inv( Z.T.dot(W).dot(Z) ) ).dot(Z.T).dot(W)\n",
    "        Rz = np.identity(n_subj) - ZZ\n",
    "        Y = np.diag(sf).dot(Rz).dot(Y)\n",
    "        \n",
    "        ## Perform WLS.\n",
    "        Bmap[n,m], Fmap[n,m] = wls(X,Y,W) \n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Save results.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#  \n",
    "\n",
    "def prepare_image(arr, space):\n",
    "    npz = np.load('fmri_second_levels/%s_connectivity.npz' %space)\n",
    "    image = np.zeros_like(npz['mapping'], dtype=float)\n",
    "    \n",
    "    if not space == 'mni305': \n",
    "        image[npz['vertices']] += arr\n",
    "    else:\n",
    "        x,y,z = npz['voxels'].T\n",
    "        image[x,y,z] += arr\n",
    "    \n",
    "    for _ in range(4 - len(image.shape)): image = np.expand_dims(image,-1)\n",
    "    return image\n",
    "\n",
    "## Translate array back into proper space.\n",
    "image = prepare_image(Fmap.squeeze(), space).squeeze()\n",
    "\n",
    "## Load in results table.\n",
    "if space == 'mni305':\n",
    "    results = read_csv('fmri_second_levels/labels/seeds_%s/%s_volume_mni2.csv' %(contrast,contrast))\n",
    "    fscores = [image[i,j,k] for i,j,k in results[['cX','cY','cZ']].as_matrix()]\n",
    "    results['Fpre'] = fscores\n",
    "    results.to_csv('fmri_second_levels/labels/seeds_%s/%s_volume_mni2.csv' %(contrast,contrast))\n",
    "else:\n",
    "    results = read_csv('fmri_second_levels/labels/seeds_%s/%s_surface_mni2.csv' %(contrast,contrast))\n",
    "    if not 'Fpre' in results.columns: results['Fpre'] = np.nan\n",
    "    vertices = results.loc[[True if label.endswith(space) else False for label in results.Label],'V'].as_matrix()\n",
    "    for v in vertices: results.loc[results.V==v,'Fpre'] = image[v]\n",
    "    results.to_csv('fmri_second_levels/labels/seeds_%s/%s_surface_mni2.csv' %(contrast,contrast), index=False)\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Connectivity Analysis\n",
    "This was exploratory analysis and ultimately dropped from the manuscript. Code is left in to document its existence and as a template for future projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from mne import read_label\n",
    "from pandas import read_csv\n",
    "from scipy.signal import detrend\n",
    "mri_dir = '/autofs/space/lilli_002/users/DARPA-ARC'\n",
    "label_dir = 'fmri_second_levels/labels'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "results_file = 'arc_hierarchical_add_FINAL_regressors'\n",
    "sm = 6\n",
    "fd = 0.9\n",
    "\n",
    "## MRI parameters.\n",
    "n_acq = 977\n",
    "tr = 1.75\n",
    "n_extract = 8\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Prepare regressors.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load trial information.\n",
    "results_file = 'stan_results/%s.csv' %results_file\n",
    "df = read_csv(results_file)\n",
    "subjects = np.unique(df.Subject)\n",
    "\n",
    "## Load DCT basis set.\n",
    "dct = np.loadtxt('fmri_second_levels/dct_0.02.txt')\n",
    "\n",
    "## Define TR onsets.\n",
    "tr_onsets = np.insert( np.cumsum( np.ones(n_acq - 1) * tr ), 0, 0 )\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "ROIs = sorted([f for f in os.listdir(label_dir) if f.endswith('label') or f.endswith('npy')])\n",
    "names = [roi for roi in ROIs if '-lh.label' in roi] +\\\n",
    "        [roi for roi in ROIs if '-rh.label' in roi] +\\\n",
    "        [roi for roi in ROIs if 'npy' in roi]\n",
    "names = [n.split('.')[0] for n in names]\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    print subject,\n",
    "    subj_dir = os.path.join(mri_dir, subject, 'arc_001', '001')\n",
    "    \n",
    "    ## Find onsets of trials relative to acquisitions.\n",
    "    trial_onsets = df.loc[df.Subject==subject, 'RiskOnset'].as_matrix()\n",
    "    onsets = np.digitize(trial_onsets, tr_onsets) - 1\n",
    "    \n",
    "    ## Load motion regressors / censors.\n",
    "    mc = np.loadtxt(os.path.join(subj_dir, 'FINAL.mc.par'))\n",
    "    try:\n",
    "        censor = np.loadtxt(os.path.join(subj_dir, 'FINAL.censor.%s.par' %fd))\n",
    "        exclude = np.in1d(tr_onsets, censor)\n",
    "    except IOError:\n",
    "        exclude = np.zeros_like(tr_onsets).astype(bool)\n",
    "    \n",
    "    ## Determine inclusion.\n",
    "    include = np.zeros(n_acq)\n",
    "    for onset in onsets: include[onset:(onset+n_extract)] += 1\n",
    "    include *= np.invert(exclude)\n",
    "    \n",
    "    extractions = np.zeros((len(ROIs), include.astype(int).sum()))\n",
    "    \n",
    "    n = 0\n",
    "    for space in ['lh','rh','mni305']:\n",
    "        \n",
    "        ## Load MRI data.\n",
    "        if space == 'mni305': f = 'fmcpr.sm%s.%s.2mm.b0dc.nii.gz' %(sm,space)\n",
    "        else: f = 'fmcpr.sm%s.fsaverage.%s.b0dc.nii.gz' %(sm,space)\n",
    "        data = nib.load(os.path.join(subj_dir, f)).get_data().squeeze()\n",
    "        \n",
    "        ## Setup ROIs.\n",
    "        if space == 'lh': \n",
    "            rois = [roi for roi in ROIs if roi.endswith('-lh.label')]\n",
    "            indices = [read_label(os.path.join(label_dir,roi)).vertices for roi in rois]\n",
    "        elif space == 'rh': \n",
    "            rois = [roi for roi in ROIs if roi.endswith('-rh.label')]\n",
    "            indices = [read_label(os.path.join(label_dir,roi)).vertices for roi in rois]\n",
    "        else: \n",
    "            rois = [roi for roi in ROIs if roi.endswith('npy')]\n",
    "            indices = [np.load(os.path.join(label_dir, roi)) for roi in rois]\n",
    "            indices = [[arr for arr in ix.T] for ix in indices]\n",
    "\n",
    "        ## Iterate over ROIs.\n",
    "        for roi, ix in zip(rois, indices):\n",
    "            \n",
    "            ## Extract timecourse.\n",
    "            ltc = data[ix].mean(axis=0)\n",
    "            ltc = ltc[np.where(include)]\n",
    "            \n",
    "            ## Detrend.\n",
    "            ltc = detrend(ltc, type='linear')\n",
    "            ltc = detrend(ltc, type='constant')\n",
    "            \n",
    "            ## Store.\n",
    "            extractions[n] = ltc\n",
    "            n += 1\n",
    "        \n",
    "    ## Save.\n",
    "    f = 'fmri_second_levels/labels/extractions/%s_extractions' %subject\n",
    "    np.savez_compressed(f, data=extractions, mc=mc[np.where(include)], \n",
    "                        dct=dct[np.where(include)], labels=np.array(names))\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "extractions_dir = 'fmri_second_levels/labels/extractions/'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "n_regressors = []\n",
    "files = sorted(os.listdir(extractions_dir))\n",
    "for n, f in enumerate(files):\n",
    "    \n",
    "    ## Load data. Make subject index.\n",
    "    npz = np.load('%s/%s' %(extractions_dir,f))\n",
    "    n_acq, n_mc = npz['mc'].shape\n",
    "    ix = np.ones(n_acq) * n\n",
    "    \n",
    "    ## Tack on rows to mc.\n",
    "    mc = np.hstack([npz['mc'], np.zeros([n_acq,6-n_mc])])\n",
    "    n_regressors.append(n_mc)\n",
    "    \n",
    "    ## Assemble data.\n",
    "    if not n:\n",
    "        data = npz['data'].T\n",
    "        motion = mc.copy()\n",
    "        dct = npz['dct']\n",
    "        index = ix.copy()\n",
    "    else:\n",
    "        data = np.concatenate([data, npz['data'].T], axis=0)\n",
    "        motion = np.concatenate([motion, mc.copy()], axis=0)\n",
    "        dct = np.concatenate([dct, npz['dct']], axis=0)\n",
    "        index = np.concatenate([index,ix])\n",
    "        \n",
    "## Finalize information.\n",
    "subjects = np.array([f.split('_')[0] for f in files])\n",
    "n_regressors = np.array(n_regressors)\n",
    "index += 1\n",
    "\n",
    "## Save.\n",
    "np.savez_compressed('fmri_second_levels/labels/extractions', subjects=subjects, index=index, data=data, \n",
    "                    dct=dct, motion=motion, n_regressors=n_regressors, labels=npz['labels'])\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle, os, pystan\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "from itertools import combinations\n",
    "sns.set_style('white')\n",
    "%matplotlib inline\n",
    "\n",
    "def zscore(arr): return (arr - arr.mean()) / arr.std()\n",
    "\n",
    "def partial_corr(corr):\n",
    "    prec = np.linalg.inv(corr)\n",
    "    I,J = np.tril_indices_from(prec,k=-1)\n",
    "    return np.array([ -prec[i,j] / np.sqrt(prec[i,i]*prec[j,j]) for i,j in zip(I,J) ])\n",
    "\n",
    "def init(N,M):\n",
    "    d = dict()\n",
    "    d['L_Sigma_mu'] = np.linalg.cholesky( (np.ones((M,M)) + np.identity(M)) / 2. )\n",
    "    d['L_Sigma'] = np.array([np.identity(M) for _ in range(N)])\n",
    "    return d\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## ROI parameters (NOTE: no pre-motor).\n",
    "labels = ['dacc-lh', 'dlpfc-lh', 'insula-lh', 'mcc-lh', 'orbitalis-lh', 'pre_sma-lh', \n",
    "          'dacc-rh', 'dlpfc-rh', 'insula-rh', 'mcc-rh', 'orbitalis-rh', 'pre_sma-rh',\n",
    "          'caudate-lh', 'hippocampus-lh', 'hippocampus-rh', 'putamen-lh','putamen-rh']\n",
    "\n",
    "## Stan parameters.\n",
    "model_name = 'partial_corr_hlkj'\n",
    "\n",
    "iter = 2000\n",
    "chains = 4\n",
    "thin = 4\n",
    "n_jobs = 2\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load data.\n",
    "npz = np.load('fmri_second_levels/labels/extractions.npz')\n",
    "data = npz['data']\n",
    "index = npz['index']\n",
    "\n",
    "## Limit to labels of interest.\n",
    "labels = np.array(labels)\n",
    "data = data[:,np.in1d(npz['labels'], labels)]\n",
    "\n",
    "for n in np.unique(index):\n",
    "    \n",
    "    ## Assemble nuisance regressors.\n",
    "    ix = np.where(index==n)[0]\n",
    "    regressors = np.hstack([npz['dct'][ix], npz['motion'][ix]])\n",
    "    regressors = regressors[:,np.where(regressors.sum(axis=0), True, False)]\n",
    "    \n",
    "    ## Perform nuisance regression.\n",
    "    beta, _, _, _ = np.linalg.lstsq(regressors, data[ix])\n",
    "    data[ix] -= np.dot(regressors,beta)\n",
    "    \n",
    "    ## Normalize (z-score).\n",
    "    data[ix] = np.apply_along_axis(zscore, 0, data[ix])\n",
    "    \n",
    "## Final preparation.\n",
    "_, nrow = np.unique(index, return_counts=True)\n",
    "N, M, T = nrow.shape[0], labels.shape[0], nrow.max()\n",
    "\n",
    "Y = np.zeros((N,T,M))\n",
    "for n, r in enumerate(nrow): Y[n,:r,:] = data[index == n+1]\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Pystan.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Assemble data.\n",
    "init = [init(N,M) for _ in range(chains)]\n",
    "data = dict(N=N, M=M, T=T, Y=Y, nrow=nrow)\n",
    "\n",
    "## Fit model.\n",
    "print 'Running Stan.',\n",
    "f = 'stan_models/%s.txt' %model_name\n",
    "fit = pystan.stan(f, data=data, chains=chains, iter=iter, thin=thin, init=init, \n",
    "                  seed=47404, n_jobs=n_jobs)\n",
    "print 'Finished.'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Save summary file.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "summary = fit.summary()\n",
    "summary = DataFrame(summary['summary'], columns=summary['summary_colnames'], index=summary['summary_rownames'])\n",
    "f = 'stan_results/%s.csv' %model_name\n",
    "summary.to_csv(f)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Extract Results.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "results = fit.extract()\n",
    "\n",
    "## Append data to results.\n",
    "results['Subjects'] = npz['subjects']\n",
    "results['Labels'] = labels\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Save results.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "print 'Saving data.'\n",
    "\n",
    "## Save all data.\n",
    "f = 'stan_results/%s.pickle' %model_name\n",
    "with open(f, 'wb') as f: cPickle.dump(results, f)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Extract group correlation matrix.\n",
    "sigma_mu = results['Sigma_mu']\n",
    "\n",
    "## Contrast partial correlation matrices.\n",
    "# data = np.array([partial_corr(corr) for corr in sigma_mu[:,ix][:,:,ix]])\n",
    "data = np.array([partial_corr(corr) for corr in sigma_mu])\n",
    "pairs = np.array(list(combinations(labels,2)))\n",
    "\n",
    "## Sort items\n",
    "ix = np.argsort(np.median(data, axis=0))[::-1]\n",
    "data = data[:,ix]\n",
    "pairs = pairs[ix]\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(12,6))\n",
    "sns.violinplot(data=data, cut=0, ax=ax)\n",
    "ax.hlines(0, 0, len(pairs), color='c', alpha=0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "718px",
    "left": "0px",
    "right": "1089px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "toc_position": {
   "height": "767px",
   "left": "0px",
   "right": "1317px",
   "top": "106px",
   "width": "205px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
