{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DARPA-ARC Notebook 1: Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T21:32:22.837001Z",
     "start_time": "2019-04-11T21:32:22.810197Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.94444444444444 8.799170235050394\n",
      "M    23\n",
      "F    13\n",
      "Name: Gender, dtype: int64\n",
      "M    18\n",
      "F    10\n",
      "Name: Gender, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "import os.path as op\n",
    "\n",
    "mri_dir = '/autofs/space/lilli_002/users/JNeurosci_ARC/'\n",
    "\n",
    "## Load demographics data.\n",
    "df = read_csv(op.join(mri_dir, 'demographics.csv'))\n",
    "\n",
    "## Calculate average age.\n",
    "print(df.Age.mean(), df.Age.std())\n",
    "\n",
    "## Tabulate genders before rejection.\n",
    "print(df.Gender.value_counts())\n",
    "print(df[~df.Exlude].Gender.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble Behavior Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T21:33:57.604006Z",
     "start_time": "2019-04-11T21:33:57.362721Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import concat, read_csv\n",
    "import os.path as op\n",
    "\n",
    "version = 'Version2090405'\n",
    "mri_dir = '/autofs/space/lilli_002/users/JNeurosci_ARC/'\n",
    "csv_dir = 'behavior'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "modality = 'mri'\n",
    "scanner_fix = 2.95\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load and assemble data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define subject list.\n",
    "df = read_csv(op.join(mri_dir, 'demographics.csv'))\n",
    "subjects = df.loc[~df.Exlude,'Subject']\n",
    "\n",
    "df = []\n",
    "for subject in subjects:\n",
    "    #\n",
    "    f = op.join(mri_dir, csv_dir, '%s_arc_%s_ser-1' %(subject,modality))\n",
    "    csv = read_csv(f)\n",
    "    #\n",
    "    ## Add subject. Remove missing trials.\n",
    "    csv['Subject'] = subject\n",
    "    #\n",
    "    ## Append.\n",
    "    df.append(csv)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Merge and preprocess.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "## Merge and crop.\n",
    "columns = ['Subject', 'Trial', 'RiskType', 'RewardType', 'Reward', 'ResponseType', \n",
    "           'ResponseTime','RiskOnset','ShockOnset']\n",
    "df = concat(df, sort=False)[columns].reset_index(drop=True)\n",
    "\n",
    "## Replace missing responses.\n",
    "df.loc[df.ResponseType==99,'ResponseType'] = np.nan\n",
    "\n",
    "## Due to code-scanner issues, we will impute all reaction times greater than 2.95s.\n",
    "if scanner_fix: df.loc[df.ResponseTime>scanner_fix, 'ResponseTime'] = np.nan\n",
    "\n",
    "## Save.\n",
    "df.to_csv(op.join(mri_dir, 'stan_results/arc_%s_%s.csv' % (modality, version)), index=False)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Models with Stan\n",
    "See [here](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations) for discussion of choice of logistic priors. Namely:\n",
    ">Assuming that nonbinary variables have been scaled to have mean 0 and standard deviation 0.5, [Gelman et al (2008)](https://arxiv.org/pdf/0901.4011.pdf) recommended *student_t(1,0,2.5)*, i.e. Cauchy distribution. Later it has been observed that this has too thick tails, so that in cases where data is not informative (e.g. in case of separation) the sampling from the posterior is challenging (see e.g. [Ghosh et al, 2015](http://arxiv.org/abs/1507.07170)). Thus Student's t distribution with higher degrees of freedom is recommended. There is not yet conclusive results what specific value should be recommended, and thus the current recommendation is to choose 3<nu<7. \n",
    "\n",
    ">Normal distribution is not recommended as a weakly informative prior, because it is not robust (see [O'Hagan (1979)](https://www.jstor.org/stable/2985064)). Normal distribution would be fine as an informative prior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T21:56:09.773653Z",
     "start_time": "2019-04-11T21:36:28.167585Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_5c4f3351ab7915a2bb1ad9dfc1b9ea82 NOW.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 1000.0\n",
      "Running Stan.\n",
      "Finished.\n",
      "Saving data.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pickle, os, pystan\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "from pandas import DataFrame, read_csv, get_dummies\n",
    "def zscore(arr): return (arr - np.nanmean(arr)) / np.nanstd(arr) \n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Specify parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## I/O parameters.\n",
    "version = 'Version20190405'\n",
    "file_name = 'arc_mri_%s.csv' % version\n",
    "model_name = 'arc_hierarchical_%s' % version\n",
    "out_name = 'arc_hierarchical_%s' % version\n",
    "interactions = False\n",
    "mri_dir = '/autofs/space/lilli_002/users/JNeurosci_ARC/'\n",
    "\n",
    "## Model parameters\n",
    "nu = 5\n",
    "\n",
    "## Sampling parameters.\n",
    "chains = 4\n",
    "samples = 2000\n",
    "thin = 4\n",
    "seed = 47404\n",
    "n_jobs = 2\n",
    "\n",
    "print('n_samples: %s' %(chains * samples / 2 / thin))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Assemble data for model.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load data.\n",
    "df = read_csv(op.join(mri_dir, 'stan_results/%s' % file_name))\n",
    "df = df[~np.isnan(df.ResponseType)].reset_index(drop=True)\n",
    "\n",
    "## Make subject index.\n",
    "subjects, ix = np.unique(df.Subject, return_inverse=True)        \n",
    "ix += 1\n",
    "\n",
    "## Make missing data index.\n",
    "mi = df.ResponseTime.isnull().astype(int)\n",
    "mi *= np.cumsum(mi)\n",
    "\n",
    "## Assemble indepedent variables.\n",
    "_, med_risk, high_risk = get_dummies(df.RiskType).values.T \n",
    "risk = np.vstack([med_risk, high_risk])\n",
    "intercept = np.ones_like(med_risk)\n",
    "reward = df.Reward.values                    \n",
    "\n",
    "if interactions: X = np.vstack([intercept,risk,reward,risk*reward]).T \n",
    "else: X = np.vstack([intercept,risk,reward]).T  \n",
    "\n",
    "## Assemble depedent variables.\n",
    "N = df.ResponseType.values.astype(int)\n",
    "Z = df.ResponseTime.values\n",
    "Z[np.isnan(Z)] = 99\n",
    "\n",
    "## Z-score variables.\n",
    "zX = X.copy()\n",
    "if not interactions: \n",
    "    zX[:,-1] = zscore(X[:,-1])\n",
    "else:\n",
    "    zX[:,3] = zscore(zX[:,3])\n",
    "    zX[:,4] = zX[:,1] * zX[:,3]\n",
    "    zX[:,5] = zX[:,2] * zX[:,3]\n",
    "\n",
    "## Assemble metadata.\n",
    "n_obs, n_pred = X.shape\n",
    "n_subj = subjects.shape[0]\n",
    "n_miss = len(mi.nonzero()[0])\n",
    "\n",
    "## Assemble data.\n",
    "data = dict(n_obs=n_obs, n_pred=n_pred, n_subj=n_subj, n_miss=n_miss, ix=ix, mi=mi, X=zX, N=N, Z=Z)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Perform Bayesian modeling.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "print('Running Stan.')\n",
    "f = op.join(mri_dir, 'stan_models/%s.txt' % model_name)\n",
    "fit = pystan.stan(file=f, data=data, chains=chains, iter=samples, thin=thin,\n",
    "                  seed=seed, n_jobs=n_jobs)\n",
    "print('Finished.')\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Save summary file.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "summary = fit.summary()\n",
    "summary = DataFrame(summary['summary'], columns=summary['summary_colnames'], index=summary['summary_rownames'])\n",
    "f = op.join(mri_dir, 'stan_results/%s.csv' % out_name)\n",
    "summary.to_csv(f)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Extract Results.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "results = fit.extract()\n",
    "\n",
    "## Append data to results.\n",
    "results['Subjects'] = subjects\n",
    "results['X'] = X\n",
    "results['zX'] = zX\n",
    "results['N'] = N\n",
    "results['Z'] = Z\n",
    "results['ix'] = ix\n",
    "results['RiskOnset'] = df.RiskOnset.values\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Save results.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "print('Saving data.')\n",
    "\n",
    "## Save all data.\n",
    "f = op.join(mri_dir, 'stan_results/%s.pickle' % out_name)\n",
    "with open(f, 'wb') as f: pickle.dump(results, f)\n",
    "    \n",
    "## Save log-likelihood for R.\n",
    "np.savetxt(op.join(mri_dir, 'stan_results/%s_loglik.txt' % out_name), np.log(results[u'PointPosteriors']))\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Predictive Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T22:01:40.232732Z",
     "start_time": "2019-04-11T22:00:57.723114Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data.\n",
      "Evaluating choice data.\n",
      "Evaluating reaction time data.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pickle, os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import os.path as op\n",
    "from pandas import DataFrame\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "version = 'Version20190405'\n",
    "mri_dir = '/autofs/space/lilli_002/users/JNeurosci_ARC/'\n",
    "results_file = 'arc_hierarchical_%s' % version\n",
    "ncol = 4\n",
    "decim = 4\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Open results file.\n",
    "print('Loading data.')\n",
    "f = op.join(mri_dir, 'stan_results/%s.pickle' % results_file)\n",
    "with open(f, 'rb') as f: results = pickle.load(f)\n",
    "\n",
    "## Extract variables.\n",
    "subjects = results['Subjects']\n",
    "ix = results['ix'] - 1\n",
    "n_subjects = ix.max() + 1\n",
    "nrow = int(np.ceil(n_subjects / ncol))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Evaluate fit to choice data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#   \n",
    "\n",
    "print('Evaluating choice data.')\n",
    "\n",
    "def bernoulli(p): return np.random.binomial(1,p,size=1)\n",
    "\n",
    "\n",
    "Bernoulli = np.vectorize(bernoulli)\n",
    "\n",
    "## Compute true ratio of takes.\n",
    "ratio_obs = np.array([results['N'][ix==i].mean() for i in range(n_subjects)])\n",
    "\n",
    "## Generate simulations of take/no-take.\n",
    "np.random.seed(47404)\n",
    "sN = Bernoulli(results['theta'])\n",
    "\n",
    "## Compute ratio take from simulations.\n",
    "ratio_sim = np.array([sN[:,ix==i].mean(axis=1) for i in range(n_subjects)])\n",
    "\n",
    "## Compute difference between observed and median of distribution.\n",
    "ppc = ratio_obs - np.median(ratio_sim, axis=1)\n",
    "\n",
    "## Compute group statistic.\n",
    "rms_ppc = np.sqrt(np.power(ppc,2).mean())\n",
    "\n",
    "## Initialize plots.\n",
    "fig, axes = plt.subplots(nrow,ncol,figsize=(nrow*3,ncol*4),sharey=True)\n",
    "\n",
    "for n, subject in enumerate(subjects):\n",
    "    #\n",
    "    axes[int(n/ncol),n%ncol].hist(ratio_sim[n], bins=8, color='#7ec0ee')\n",
    "    axes[int(n/ncol),n%ncol].vlines(ratio_obs[n], 0, 500, linewidth=2, linestyle='--')\n",
    "    axes[int(n/ncol),n%ncol].set_title(subject.upper())\n",
    "    x1, x2 = axes[int(n/ncol),n%ncol].get_xlim()\n",
    "    xticks = np.linspace(x1,x2,3).round(2)\n",
    "    axes[int(n/ncol),n%ncol].set_xticks(xticks)\n",
    "    \n",
    "\n",
    "plt.suptitle('PPC = %0.3f' %rms_ppc, y=0.99)\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(op.join(mri_dir, 'plots/ppc/%s_ppc_choice.png' % results_file))\n",
    "plt.close('all')\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Evaluate fit to reaction time data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#  \n",
    "\n",
    "print('Evaluating reaction time data.')\n",
    "\n",
    "## Extract and provide imputations data.\n",
    "Z = results['Z'].copy()\n",
    "Z[np.where(Z==99)] = results['Zm'].mean(axis=0)\n",
    "\n",
    "## Initialize plots.\n",
    "fig, axes = plt.subplots(nrow,ncol,figsize=(nrow*3,ncol*4))\n",
    "\n",
    "for n, subject in enumerate(subjects):\n",
    "    #\n",
    "    ## Extract info.\n",
    "    a0 = results['a0'][:,n]\n",
    "    if 'non-' in results_file: a1 = results['a1_mu']\n",
    "    else: a1 = results['a1'][:,n]\n",
    "    shape = results['shape'][:,n]\n",
    "    ddb = results['ddb'][:,ix==n]\n",
    "    #\n",
    "    ## Calculate gamma parameters.\n",
    "    mu = a0 + (a1 * ddb.T) \n",
    "    scale = mu / shape\n",
    "    #\n",
    "    ## Simulate reaction times.\n",
    "    np.random.seed(47404)\n",
    "    def gamma(shape,scale): return np.random.gamma(shape,scale,size=1)\n",
    "    Gamma = np.vectorize(gamma)\n",
    "    rt_sim = Gamma(shape,scale)\n",
    "    #\n",
    "    ## Extract observed reaction times.\n",
    "    rt_obs = Z[ix==n]\n",
    "    #\n",
    "    ## Plot observed.\n",
    "    density, bins = np.histogram(rt_obs, bins=5, density=True)\n",
    "    x = bins[:-1] + np.diff(bins) \n",
    "    axes[int(n/ncol),n%ncol].plot(x,density,linewidth=3,color='#7ec0ee')\n",
    "    axes[int(n/ncol),n%ncol].set_title(subject.upper())\n",
    "    axes[int(n/ncol),n%ncol].set_xlim(0,5)\n",
    "    #\n",
    "    ## Plot simulated.\n",
    "    for arr in rt_sim.T[::decim]:\n",
    "        density, bins = np.histogram(arr, bins=5, density=True)\n",
    "        x = bins[:-1] + np.diff(bins) \n",
    "        axes[int(n/ncol),n%ncol].plot(x,density,color='k',alpha=0.01)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(op.join(mri_dir, 'plots/ppc/%s_ppc_rt.png' % results_file))\n",
    "plt.close('all')\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison: Compute WAIC / CV-LOO\n",
    "Performed in R. See other script.\n",
    "\n",
    "### Assemble model outputs for fMRI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T22:07:36.687981Z",
     "start_time": "2019-04-11T22:07:36.466719Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data.\n",
      "Assembling dataframe.\n",
      "Saving dataframe.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pickle, os\n",
    "import numpy as np\n",
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "version = 'Version20190405'\n",
    "mri_dir = '/autofs/space/lilli_002/users/JNeurosci_ARC/'\n",
    "results_file = 'arc_hierarchical_%s' % version\n",
    "csv_file = 'arc_mri_%s' % version\n",
    "out_file = 'arc_hierarchical_%s_regressors' % version\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Open results file.\n",
    "print('Loading data.')\n",
    "f = op.join(mri_dir, 'stan_results/%s.pickle' % results_file)\n",
    "with open(f, 'rb') as f: results = pickle.load(f)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Build dataframe.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "print('Assembling dataframe.')\n",
    "\n",
    "## Initialize.\n",
    "df = dict()\n",
    "\n",
    "## Add subject info.\n",
    "ix = results['ix'] - 1\n",
    "df['Subject'] = results['Subjects'][ix]\n",
    "\n",
    "## Add regressors.\n",
    "df['RiskType'] = np.where(results['X'][:,1]==1, 2, np.where(results['X'][:,2]==1,3,1))\n",
    "df['Reward']   = results['X'][:,3]\n",
    "df['theta']    = np.median(results['theta'], axis=0)\n",
    "df['ddb']      = 0.25 - np.power(df['theta'] - 0.5, 2)\n",
    "\n",
    "## Extract and provide imputations data.\n",
    "Z = results['Z'].copy()\n",
    "Z[np.where(Z==99)] = results['Zm'].mean(axis=0)\n",
    "df['RT'] = Z\n",
    "df['RiskOnset'] = results['RiskOnset']\n",
    "\n",
    "## Assemble dataframe.\n",
    "df = DataFrame(df)\n",
    "\n",
    "## Merge with original file.\n",
    "csv = read_csv(op.join(mri_dir, 'stan_results/%s.csv' % csv_file))\n",
    "csv.drop('ResponseTime', axis=1, inplace=True)\n",
    "\n",
    "df = df.merge(csv, how='outer', on=['Subject','RiskType','Reward','RiskOnset'])\n",
    "df = df.sort_values(['Subject','Trial']).reset_index(drop=True)\n",
    "df = df[['Subject','Trial','RiskType','Reward','ResponseType','ddb','RT','RiskOnset','ShockOnset']]\n",
    "\n",
    "## Save.\n",
    "print('Saving dataframe.')\n",
    "df.to_csv(op.join(mri_dir, 'stan_results/%s.csv' % out_file), index=False)\n",
    "\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "175px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
