{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DARPA-ARC Notebook 1: Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T13:38:45.667675Z",
     "start_time": "2019-04-17T13:38:44.731713Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.94444444444444 8.799170235050394\n",
      "M    23\n",
      "F    13\n",
      "Name: Gender, dtype: int64\n",
      "M    18\n",
      "F    10\n",
      "Name: Gender, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from my_settings import df\n",
    "\n",
    "## Calculate average age.\n",
    "print(df.Age.mean(), df.Age.std())\n",
    "\n",
    "## Tabulate genders before rejection.\n",
    "print(df.Gender.value_counts())\n",
    "print(df[~df.Exlude].Gender.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble Behavior Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T13:39:09.988557Z",
     "start_time": "2019-04-17T13:39:09.720767Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from my_settings import op, version, root_dir, mri_dir, task, subjects, read_csv, np, modality, paradigm, session\n",
    "from pandas import concat\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "scanner_fix = 2.95\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load and assemble data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "df = []\n",
    "for subject in subjects:\n",
    "    #\n",
    "    f = op.join(root_dir, 'behavior', '%s_%s_%s_%s-%i' % (subject, task, modality, paradigm, session))\n",
    "    csv = read_csv(f)\n",
    "    #\n",
    "    ## Add subject. Remove missing trials.\n",
    "    csv['Subject'] = subject\n",
    "    #\n",
    "    ## Append.\n",
    "    df.append(csv)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Merge and preprocess.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "## Merge and crop.\n",
    "columns = ['Subject', 'Trial', 'RiskType', 'RewardType', 'Reward', 'ResponseType', \n",
    "           'ResponseTime','RiskOnset','ShockOnset']\n",
    "df = concat(df, sort=False)[columns].reset_index(drop=True)\n",
    "\n",
    "## Replace missing responses.\n",
    "df.loc[df.ResponseType==99,'ResponseType'] = np.nan\n",
    "\n",
    "## Due to code-scanner issues, we will impute all reaction times greater than 2.95s.\n",
    "if scanner_fix: df.loc[df.ResponseTime>scanner_fix, 'ResponseTime'] = np.nan\n",
    "\n",
    "## Save.\n",
    "df.to_csv(op.join(root_dir, 'stan_results/%s_%s_%s.csv' % (task, modality, version)), index=False)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Models with Stan\n",
    "See [here](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations) for discussion of choice of logistic priors. Namely:\n",
    ">Assuming that nonbinary variables have been scaled to have mean 0 and standard deviation 0.5, [Gelman et al (2008)](https://arxiv.org/pdf/0901.4011.pdf) recommended *student_t(1,0,2.5)*, i.e. Cauchy distribution. Later it has been observed that this has too thick tails, so that in cases where data is not informative (e.g. in case of separation) the sampling from the posterior is challenging (see e.g. [Ghosh et al, 2015](http://arxiv.org/abs/1507.07170)). Thus Student's t distribution with higher degrees of freedom is recommended. There is not yet conclusive results what specific value should be recommended, and thus the current recommendation is to choose 3<nu<7. \n",
    "\n",
    ">Normal distribution is not recommended as a weakly informative prior, because it is not robust (see [O'Hagan (1979)](https://www.jstor.org/stable/2985064)). Normal distribution would be fine as an informative prior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T16:47:33.226316Z",
     "start_time": "2019-04-16T16:22:07.281711Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_5c4f3351ab7915a2bb1ad9dfc1b9ea82 NOW.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 1000.0\n",
      "Running Stan.\n",
      "Finished.\n",
      "Saving data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_7eb53f59ac79bdaef2b52b436f2d5e6e NOW.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Stan.\n",
      "Finished.\n",
      "Saving data.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from my_settings import os, op, np, read_csv, version, root_dir, mri_dir, task, subjects, stan_models, DataFrame, pickle, modality\n",
    "import pystan\n",
    "from pandas import get_dummies\n",
    "\n",
    "def zscore(arr): return (arr - np.nanmean(arr)) / np.nanstd(arr) \n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Specify parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Model parameters\n",
    "interactions = False\n",
    "nu = 5\n",
    "\n",
    "## Sampling parameters.\n",
    "chains = 4\n",
    "samples = 2000\n",
    "thin = 4\n",
    "seed = 47404\n",
    "n_jobs = 2\n",
    "\n",
    "print('n_samples: %s' %(chains * samples / 2 / thin))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Assemble data for model.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load data.\n",
    "df = read_csv(op.join(root_dir, 'stan_results/%s_%s_%s.csv' % (task, modality, version)))\n",
    "df = df[~np.isnan(df.ResponseType)].reset_index(drop=True)\n",
    "\n",
    "## Make subject index.\n",
    "subjects, ix = np.unique(df.Subject, return_inverse=True)        \n",
    "ix += 1\n",
    "\n",
    "## Make missing data index.\n",
    "mi = df.ResponseTime.isnull().astype(int)\n",
    "mi *= np.cumsum(mi)\n",
    "\n",
    "## Assemble indepedent variables.\n",
    "_, med_risk, high_risk = get_dummies(df.RiskType).values.T \n",
    "risk = np.vstack([med_risk, high_risk])\n",
    "intercept = np.ones_like(med_risk)\n",
    "reward = df.Reward.values                    \n",
    "\n",
    "if interactions: X = np.vstack([intercept, risk, reward, risk*reward]).T \n",
    "else: X = np.vstack([intercept, risk, reward]).T  \n",
    "\n",
    "## Assemble depedent variables.\n",
    "N = df.ResponseType.values.astype(int)\n",
    "Z = df.ResponseTime.values\n",
    "Z[np.isnan(Z)] = 99\n",
    "\n",
    "## Z-score variables.\n",
    "zX = X.copy()\n",
    "if not interactions: \n",
    "    zX[:,-1] = zscore(X[:,-1])\n",
    "else:\n",
    "    zX[:,3] = zscore(zX[:,3])\n",
    "    zX[:,4] = zX[:,1] * zX[:,3]\n",
    "    zX[:,5] = zX[:,2] * zX[:,3]\n",
    "\n",
    "## Assemble metadata.\n",
    "n_obs, n_pred = X.shape\n",
    "n_subj = subjects.shape[0]\n",
    "n_miss = len(mi.nonzero()[0])\n",
    "\n",
    "## Assemble data.\n",
    "data = dict(n_obs=n_obs, n_pred=n_pred, n_subj=n_subj, n_miss=n_miss, ix=ix, mi=mi, X=zX, N=N, Z=Z)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Perform Bayesian modeling.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for model_name in stan_models:\n",
    "    print('Running Stan.')\n",
    "    f = op.join(root_dir, 'stan_models/%s_%s.txt' % (task, model_name))\n",
    "    fit = pystan.stan(file=f, data=data, chains=chains, iter=samples, thin=thin,\n",
    "                      seed=seed, n_jobs=n_jobs)\n",
    "    print('Finished.')\n",
    "    #\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ## Save summary file.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    #\n",
    "    summary = fit.summary()\n",
    "    summary = DataFrame(summary['summary'], columns=summary['summary_colnames'], index=summary['summary_rownames'])\n",
    "    f = op.join(root_dir, 'stan_results/%s_%s_%s.csv' % (version, task, model_name))\n",
    "    summary.to_csv(f)\n",
    "    #\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ## Extract Results.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    #\n",
    "    results = fit.extract()\n",
    "    #\n",
    "    ## Append data to results.\n",
    "    results['Subjects'] = subjects\n",
    "    results['X'] = X\n",
    "    results['zX'] = zX\n",
    "    results['N'] = N\n",
    "    results['Z'] = Z\n",
    "    results['ix'] = ix\n",
    "    results['RiskOnset'] = df.RiskOnset.values\n",
    "    #\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ## Save results.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    print('Saving data.')\n",
    "    #\n",
    "    ## Save all data.\n",
    "    f = op.join(root_dir, 'stan_results/%s_%s_%s.pickle' % (version, task, model_name))\n",
    "    with open(f, 'wb') as f: pickle.dump(results, f)\n",
    "    #\n",
    "    ## Save log-likelihood for R.\n",
    "    np.savetxt(op.join(root_dir, 'stan_results/%s_%s_%s_loglik.txt' % (version, task, model_name)), np.log(results[u'PointPosteriors']))\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Predictive Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T17:09:22.633933Z",
     "start_time": "2019-04-16T17:07:55.377582Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for arc_hierarchical.\n",
      "Evaluating choice data.\n",
      "Evaluating reaction time data.\n",
      "Done.\n",
      "Loading data for arc_non-hierarchical.\n",
      "Evaluating choice data.\n",
      "Evaluating reaction time data.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from my_settings import os, op, np, read_csv, version, root_dir, mri_dir, task, subjects, stan_models, plt, DataFrame, pickle\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "ncol = 4\n",
    "decim = 4\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Open results file.\n",
    "for model_name in stan_models:\n",
    "    print('Loading data for %s.' % model_name)\n",
    "    f = op.join(root_dir, 'stan_results/%s_%s_%s.pickle' % (version, task, model_name))\n",
    "    with open(f, 'rb') as f: results = pickle.load(f)\n",
    "    #\n",
    "    ## Extract variables.\n",
    "    subjects = results['Subjects']\n",
    "    ix = results['ix'] - 1\n",
    "    n_subjects = ix.max() + 1\n",
    "    nrow = int(np.ceil(n_subjects / ncol))\n",
    "    #\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Evaluate fit to choice data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#   \n",
    "    #\n",
    "    print('Evaluating choice data.')\n",
    "    #\n",
    "    def bernoulli(p): return np.random.binomial(1,p,size=1)\n",
    "    #\n",
    "    Bernoulli = np.vectorize(bernoulli)\n",
    "    #\n",
    "    ## Compute true ratio of takes.\n",
    "    ratio_obs = np.array([results['N'][ix==i].mean() for i in range(n_subjects)])\n",
    "    #\n",
    "    ## Generate simulations of take/no-take.\n",
    "    np.random.seed(47404)\n",
    "    sN = Bernoulli(results['theta'])\n",
    "    #\n",
    "    ## Compute ratio take from simulations.\n",
    "    ratio_sim = np.array([sN[:,ix==i].mean(axis=1) for i in range(n_subjects)])\n",
    "    #\n",
    "    ## Compute difference between observed and median of distribution.\n",
    "    ppc = ratio_obs - np.median(ratio_sim, axis=1)\n",
    "    #\n",
    "    ## Compute group statistic.\n",
    "    rms_ppc = np.sqrt(np.power(ppc,2).mean())\n",
    "    #\n",
    "    ## Initialize plots.\n",
    "    fig, axes = plt.subplots(nrow,ncol,figsize=(nrow*3,ncol*4),sharey=True)\n",
    "    #\n",
    "    for n, subject in enumerate(subjects):\n",
    "        #\n",
    "        axes[int(n/ncol),n%ncol].hist(ratio_sim[n], bins=8, color='#7ec0ee')\n",
    "        axes[int(n/ncol),n%ncol].vlines(ratio_obs[n], 0, 500, linewidth=2, linestyle='--')\n",
    "        axes[int(n/ncol),n%ncol].set_title(subject.upper())\n",
    "        x1, x2 = axes[int(n/ncol),n%ncol].get_xlim()\n",
    "        xticks = np.linspace(x1,x2,3).round(2)\n",
    "        axes[int(n/ncol),n%ncol].set_xticks(xticks)\n",
    "    #\n",
    "    plt.suptitle('PPC = %0.3f' %rms_ppc, y=0.99)\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    plt.savefig(op.join(root_dir, 'plots/ppc/%s_%s_%s_ppc_choice.png' % (version, task, model_name)))\n",
    "    plt.close('all')\n",
    "    #\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Evaluate fit to reaction time data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#  \n",
    "    #\n",
    "    print('Evaluating reaction time data.')\n",
    "    #\n",
    "    ## Extract and provide imputations data.\n",
    "    Z = results['Z'].copy()\n",
    "    Z[np.where(Z==99)] = results['Zm'].mean(axis=0)\n",
    "    #\n",
    "    ## Initialize plots.\n",
    "    fig, axes = plt.subplots(nrow,ncol,figsize=(nrow*3,ncol*4))\n",
    "    #\n",
    "    for n, subject in enumerate(subjects):\n",
    "        #\n",
    "        ## Extract info.\n",
    "        a0 = results['a0'][:,n]\n",
    "        if 'non-' in model_name: a1 = results['a1_mu']\n",
    "        else: a1 = results['a1'][:,n]\n",
    "        shape = results['shape'][:,n]\n",
    "        ddb = results['ddb'][:,ix==n]\n",
    "        #\n",
    "        ## Calculate gamma parameters.\n",
    "        mu = a0 + (a1 * ddb.T) \n",
    "        scale = mu / shape\n",
    "        #\n",
    "        ## Simulate reaction times.\n",
    "        np.random.seed(47404)\n",
    "        def gamma(shape,scale): return np.random.gamma(shape,scale,size=1)\n",
    "        Gamma = np.vectorize(gamma)\n",
    "        rt_sim = Gamma(shape,scale)\n",
    "        #\n",
    "        ## Extract observed reaction times.\n",
    "        rt_obs = Z[ix==n]\n",
    "        #\n",
    "        ## Plot observed.\n",
    "        density, bins = np.histogram(rt_obs, bins=5, density=True)\n",
    "        x = bins[:-1] + np.diff(bins) \n",
    "        axes[int(n/ncol),n%ncol].plot(x,density,linewidth=3,color='#7ec0ee')\n",
    "        axes[int(n/ncol),n%ncol].set_title(subject.upper())\n",
    "        axes[int(n/ncol),n%ncol].set_xlim(0,5)\n",
    "        #\n",
    "        ## Plot simulated.\n",
    "        for arr in rt_sim.T[::decim]:\n",
    "            density, bins = np.histogram(arr, bins=5, density=True)\n",
    "            x = bins[:-1] + np.diff(bins) \n",
    "            axes[int(n/ncol),n%ncol].plot(x,density,color='k',alpha=0.01)\n",
    "    #\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    plt.savefig(op.join(root_dir, 'plots/ppc/%s_%s_ppc_rt.png' % (version, model_name)))\n",
    "    plt.close('all')\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison: Compute WAIC / CV-LOO\n",
    "Performed in R. See other script.\n",
    "\n",
    "### Assemble model outputs for fMRI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T13:40:55.506593Z",
     "start_time": "2019-04-17T13:40:54.826701Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data.\n",
      "Assembling dataframe.\n",
      "Saving dataframe.\n",
      "Loading data.\n",
      "Assembling dataframe.\n",
      "Saving dataframe.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from my_settings import os, op, np, read_csv, version, root_dir, mri_dir, task, subjects, stan_models, categories, merge_on, DataFrame, pickle, modality\n",
    "\n",
    "for model_name in stan_models:\n",
    "    #\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    #\n",
    "    ## Open results file.\n",
    "    print('Loading data.')\n",
    "    f = op.join(root_dir, 'stan_results/%s_%s_%s.pickle' % (version, task, model_name))\n",
    "    with open(f, 'rb') as f: results = pickle.load(f)\n",
    "    #\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Build dataframe.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    #\n",
    "    print('Assembling dataframe.')\n",
    "    #\n",
    "    ## Initialize.\n",
    "    df = dict()\n",
    "    #\n",
    "    ## Add subject info.\n",
    "    ix = results['ix'] - 1\n",
    "    df['Subject'] = results['Subjects'][ix]\n",
    "    #\n",
    "    ## Add regressors.\n",
    "    df['RiskType'] = np.where(results['X'][:,1]==1, 2, np.where(results['X'][:,2]==1,3,1))\n",
    "    df['Reward']   = results['X'][:,3]\n",
    "    df['theta']    = np.median(results['theta'], axis=0)\n",
    "    df['ddb']      = 0.25 - np.power(df['theta'] - 0.5, 2)\n",
    "    #\n",
    "    ## Extract and provide imputations data.\n",
    "    Z = results['Z'].copy()\n",
    "    Z[np.where(Z==99)] = results['Zm'].mean(axis=0)\n",
    "    df['RT'] = Z\n",
    "    df['RiskOnset'] = results['RiskOnset']\n",
    "    #\n",
    "    ## Assemble dataframe.\n",
    "    df = DataFrame(df)\n",
    "    #\n",
    "    ## Merge with original file.\n",
    "    csv = read_csv(op.join(root_dir, 'stan_results/%s_%s_%s.csv' % (task, modality, version)))\n",
    "    csv.drop('ResponseTime', axis=1, inplace=True)\n",
    "    #\n",
    "    df = df.merge(csv, how='outer', on=merge_on)\n",
    "    df = df.sort_values(['Subject','Trial']).reset_index(drop=True)\n",
    "    df = df[categories]\n",
    "    #\n",
    "    ## Save.\n",
    "    print('Saving dataframe.')\n",
    "    df.to_csv(op.join(root_dir, 'stan_results/%s_%s_%s_regressors.csv' % (task, model_name, version)), index=False)\n",
    "\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "175px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
